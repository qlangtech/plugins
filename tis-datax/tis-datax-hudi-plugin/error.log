22/02/12 19:07:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
log4j:WARN No appenders could be found for logger (org.apache.hudi.utilities.deltastreamer.SchedulerConfGenerator).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/02/12 19:07:27 INFO SparkContext: Running Spark version 2.4.4
22/02/12 19:07:27 INFO SparkContext: Submitted application: delta-streamer-customer_order_relation/testDataX4465
22/02/12 19:07:27 INFO SecurityManager: Changing view acls to: mozhenghua
22/02/12 19:07:27 INFO SecurityManager: Changing modify acls to: mozhenghua
22/02/12 19:07:27 INFO SecurityManager: Changing view acls groups to: 
22/02/12 19:07:27 INFO SecurityManager: Changing modify acls groups to: 
22/02/12 19:07:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(mozhenghua); groups with view permissions: Set(); users  with modify permissions: Set(mozhenghua); groups with modify permissions: Set()
22/02/12 19:07:27 INFO deprecation: mapred.output.compression.codec is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.codec
22/02/12 19:07:27 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
22/02/12 19:07:27 INFO deprecation: mapred.output.compression.type is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.type
22/02/12 19:07:28 INFO Utils: Successfully started service 'sparkDriver' on port 54585.
22/02/12 19:07:28 INFO SparkEnv: Registering MapOutputTracker
22/02/12 19:07:28 INFO SparkEnv: Registering BlockManagerMaster
22/02/12 19:07:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/02/12 19:07:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/02/12 19:07:28 INFO DiskBlockManager: Created local directory at /private/var/folders/p_/5jfy_yk16kb_m48kn65vrqd40000gn/T/blockmgr-68495e96-04f0-4973-aa27-9c2deef86858
22/02/12 19:07:28 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
22/02/12 19:07:28 INFO SparkEnv: Registering OutputCommitCoordinator
22/02/12 19:07:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/02/12 19:07:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.28.150:4040
22/02/12 19:07:28 INFO SparkContext: Added JAR file:///opt/data/tis/libs/plugins/tis-datax-hudi-plugin/tis-datax-hudi-dependency/lib/hudi-utilities-bundle_2.11-0.10.0.jar at spark://192.168.28.150:54585/jars/hudi-utilities-bundle_2.11-0.10.0.jar with timestamp 1644664048459
22/02/12 19:07:28 INFO SparkContext: Added JAR file:/opt/data/tis/libs/plugins/tis-datax-hudi-plugin/tis-datax-hudi-dependency/tis-datax-hudi-dependency-dist-3.5.0.jar at spark://192.168.28.150:54585/jars/tis-datax-hudi-dependency-dist-3.5.0.jar with timestamp 1644664048460
22/02/12 19:07:28 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://sparkmaster:7077...
22/02/12 19:07:28 INFO TransportClientFactory: Successfully created connection to sparkmaster/192.168.28.201:7077 after 28 ms (0 ms spent in bootstraps)
22/02/12 19:07:28 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20220212110023-0010
22/02/12 19:07:28 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220212110023-0010/0 on worker-20220212024538-172.21.0.14-37847 (172.21.0.14:37847) with 4 core(s)
22/02/12 19:07:28 INFO StandaloneSchedulerBackend: Granted executor ID app-20220212110023-0010/0 on hostPort 172.21.0.14:37847 with 4 core(s), 1024.0 MB RAM
22/02/12 19:07:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54588.
22/02/12 19:07:28 INFO NettyBlockTransferService: Server created on 192.168.28.150:54588
22/02/12 19:07:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/02/12 19:07:28 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220212110023-0010/0 is now RUNNING
22/02/12 19:07:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.28.150, 54588, None)
22/02/12 19:07:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.28.150:54588 with 366.3 MB RAM, BlockManagerId(driver, 192.168.28.150, 54588, None)
22/02/12 19:07:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.28.150, 54588, None)
22/02/12 19:07:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.28.150, 54588, None)
22/02/12 19:07:28 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
22/02/12 19:07:28 INFO TISHoodieDeltaStreamer: dataXName:testDataX4465 has match test phrase create test stub mock for DataxWriter
22/02/12 19:07:28 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
22/02/12 19:07:29 INFO HdfsFileSystemFactory: successful create hdfs with hdfsAddress:hdfs://namenode
22/02/12 19:07:29 INFO InitReactorRunner: Started initialization
22/02/12 19:07:29 INFO InitReactorRunner: Attained Listing up plugins
22/02/12 19:07:29 INFO InitReactorRunner: Attained null
22/02/12 19:07:29 INFO InitReactorRunner: Attained null
22/02/12 19:07:29 INFO InitReactorRunner: Attained Preparing plugins
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-aliyun-fs-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-ds-mysql-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-ftp-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-kafka-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-asyncmsg-rocketmq-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-sqlserver-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-doris-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-hdfs-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-common-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-local-executor.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-flink-cdc-oracle-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-ds-tidb-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-sink-elasticsearch7-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-postgresql-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-hive-flat-table-builder-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-mongodb-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-oracle-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-flink-dependency.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-ds-mysql-v8-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-k8s-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-elasticsearch-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-oss-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-flink-cdc-mysql-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-cassandra-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-flink-cdc-postgresql-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-ds-mysql-v5-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-hudi-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-sink-starrocks-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-sink-clickhouse-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-clickhouse-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-flink-cdc-mongdb-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-realtime-flink.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-common-rdbms-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-aliyun-fs-plugin.tpi because /opt/data/tis/libs/plugins/tis-aliyun-fs-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-aliyun-fs-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-ds-mysql-plugin.tpi because /opt/data/tis/libs/plugins/tis-ds-mysql-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-ds-mysql-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-ftp-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-ftp-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-ftp-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-kafka-plugin.tpi because /opt/data/tis/libs/plugins/tis-kafka-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-kafka-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-asyncmsg-rocketmq-plugin.tpi because /opt/data/tis/libs/plugins/tis-asyncmsg-rocketmq-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-asyncmsg-rocketmq-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-sqlserver-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-sqlserver-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-sqlserver-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-doris-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-doris-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-doris-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-hdfs-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-hdfs-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-hdfs-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-common-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-common-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-common-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-local-executor.tpi because /opt/data/tis/libs/plugins/tis-datax-local-executor.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-local-executor.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-flink-cdc-oracle-plugin.tpi because /opt/data/tis/libs/plugins/tis-flink-cdc-oracle-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-flink-cdc-oracle-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-ds-tidb-plugin.tpi because /opt/data/tis/libs/plugins/tis-ds-tidb-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-ds-tidb-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-sink-elasticsearch7-plugin.tpi because /opt/data/tis/libs/plugins/tis-sink-elasticsearch7-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-sink-elasticsearch7-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-postgresql-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-postgresql-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-postgresql-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-hive-flat-table-builder-plugin.tpi because /opt/data/tis/libs/plugins/tis-hive-flat-table-builder-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-hive-flat-table-builder-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-mongodb-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-mongodb-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-mongodb-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-oracle-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-oracle-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-oracle-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-flink-dependency.tpi because /opt/data/tis/libs/plugins/tis-flink-dependency.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-flink-dependency.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-ds-mysql-v8-plugin.tpi because /opt/data/tis/libs/plugins/tis-ds-mysql-v8-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-ds-mysql-v8-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-k8s-plugin.tpi because /opt/data/tis/libs/plugins/tis-k8s-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-k8s-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-elasticsearch-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-elasticsearch-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-elasticsearch-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-oss-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-oss-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-oss-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-flink-cdc-mysql-plugin.tpi because /opt/data/tis/libs/plugins/tis-flink-cdc-mysql-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-flink-cdc-mysql-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-cassandra-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-cassandra-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-cassandra-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-flink-cdc-postgresql-plugin.tpi because /opt/data/tis/libs/plugins/tis-flink-cdc-postgresql-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-flink-cdc-postgresql-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-ds-mysql-v5-plugin.tpi because /opt/data/tis/libs/plugins/tis-ds-mysql-v5-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-ds-mysql-v5-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-hudi-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-hudi-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-hudi-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-sink-starrocks-plugin.tpi because /opt/data/tis/libs/plugins/tis-sink-starrocks-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-sink-starrocks-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-sink-clickhouse-plugin.tpi because /opt/data/tis/libs/plugins/tis-sink-clickhouse-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-sink-clickhouse-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-clickhouse-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-clickhouse-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-clickhouse-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-flink-cdc-mongdb-plugin.tpi because /opt/data/tis/libs/plugins/tis-flink-cdc-mongdb-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-flink-cdc-mongdb-plugin.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-realtime-flink.tpi because /opt/data/tis/libs/plugins/tis-realtime-flink.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-realtime-flink.tpi
22/02/12 19:07:29 INFO PluginManager: Ignoring /opt/data/tis/libs/plugins/tis-datax-common-rdbms-plugin.tpi because /opt/data/tis/libs/plugins/tis-datax-common-rdbms-plugin.tpi is already loaded
22/02/12 19:07:29 INFO InitReactorRunner: Attained Inspecting plugin /opt/data/tis/libs/plugins/tis-datax-common-rdbms-plugin.tpi
22/02/12 19:07:29 INFO InitReactorRunner: Attained Checking cyclic dependencies
22/02/12 19:07:29 INFO InitReactorRunner: Listed all plugins
22/02/12 19:07:29 INFO CenterResource: notFetchFromCenterRepository:true
22/02/12 19:07:29 INFO InitReactorRunner: Attained null
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugins
22/02/12 19:07:29 INFO InitReactorRunner: Attained null
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-aliyun-fs-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-common-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-common-rdbms-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-ds-mysql-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-ftp-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-kafka-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-asyncmsg-rocketmq-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-sqlserver-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-doris-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-hdfs-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-local-executor
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-flink-dependency
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-k8s-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-realtime-flink
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-flink-cdc-oracle-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-ds-tidb-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-elasticsearch-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-sink-elasticsearch7-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-postgresql-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-hive-flat-table-builder-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-mongodb-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-oracle-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-ds-mysql-v8-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-oss-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-flink-cdc-mysql-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-cassandra-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-flink-cdc-postgresql-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-ds-mysql-v5-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-hudi-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-sink-starrocks-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-datax-clickhouse-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-sink-clickhouse-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Loading plugin tis-flink-cdc-mongdb-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Prepared all plugins
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-aliyun-fs-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Resolving Dependant Plugins Graph
22/02/12 19:07:29 INFO InitReactorRunner: Attained null
22/02/12 19:07:29 INFO InitReactorRunner: Attained null
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-common-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-common-rdbms-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-ds-mysql-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-ftp-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-kafka-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-asyncmsg-rocketmq-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-sqlserver-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-doris-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-hdfs-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-local-executor
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-flink-dependency
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-k8s-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-realtime-flink
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-flink-cdc-oracle-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-ds-tidb-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-elasticsearch-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-sink-elasticsearch7-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-postgresql-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-hive-flat-table-builder-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-mongodb-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-oracle-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-ds-mysql-v8-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-oss-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-flink-cdc-mysql-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-cassandra-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-flink-cdc-postgresql-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-ds-mysql-v5-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-hudi-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-sink-starrocks-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-datax-clickhouse-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-sink-clickhouse-plugin
22/02/12 19:07:29 INFO InitReactorRunner: Attained Initializing plugin tis-flink-cdc-mongdb-plugin
22/02/12 19:07:30 INFO InitReactorRunner: Started all plugins
22/02/12 19:07:30 INFO InitReactorRunner: Attained Load updateCenter
22/02/12 19:07:30 INFO InitReactorRunner: Attained null
22/02/12 19:07:30 INFO InitReactorRunner: Completed initialization
22/02/12 19:07:30 INFO InitReactorRunner: Attained null
22/02/12 19:07:30 INFO TIS: tis plugin have been initialized,consume:560ms.
22/02/12 19:07:30 INFO ClassicPluginStrategy: Scout-loading ExtensionList: class com.qlangtech.tis.extension.Descriptor
22/02/12 19:07:30 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.28.201:52214) with ID 0
22/02/12 19:07:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.0.14:44157 with 366.3 MB RAM, BlockManagerId(0, 172.21.0.14, 44157, None)
22/02/12 19:07:30 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
22/02/12 19:07:30 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
22/02/12 19:07:30 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
22/02/12 19:07:30 INFO HoodieDeltaStreamer: Creating delta streamer with configs:
hoodie.bulkinsert.shuffle.parallelism: 2
hoodie.compact.inline: false
hoodie.datasource.hive_sync.database: default
hoodie.datasource.hive_sync.jdbcurl: jdbc:hive2://hiveserver:10000
hoodie.datasource.hive_sync.mode: jdbc
hoodie.datasource.hive_sync.partition_extractor_class: org.apache.hudi.hive.MultiPartKeysValueExtractor
hoodie.datasource.hive_sync.partition_fields: pt
hoodie.datasource.hive_sync.password: hive
hoodie.datasource.hive_sync.table: customer_order_relation
hoodie.datasource.hive_sync.username: hive
hoodie.datasource.write.partitionpath.field: kind
hoodie.datasource.write.reconcile.schema: false
hoodie.datasource.write.recordkey.field: customerregister_id
hoodie.delete.shuffle.parallelism: 2
hoodie.deltastreamer.csv.header: true
hoodie.deltastreamer.csv.sep: ,
hoodie.deltastreamer.schemaprovider.source.schema.file: hdfs://namenode/user/admin/default/customer_order_relation/meta/schema.avsc
hoodie.deltastreamer.schemaprovider.target.schema.file: hdfs://namenode/user/admin/default/customer_order_relation/meta/schema.avsc
hoodie.deltastreamer.source.dfs.root: hdfs://namenode/user/admin/default/customer_order_relation/data
hoodie.embed.timeline.server: true
hoodie.filesystem.view.type: EMBEDDED_KV_STORE
hoodie.insert.shuffle.parallelism: 2
hoodie.upsert.shuffle.parallelism: 2

22/02/12 19:07:30 INFO FSUtils: Resolving file hdfs://namenode/user/admin/default/customer_order_relation/meta/schema.avscto be a remote file.
22/02/12 19:07:30 INFO HoodieSparkKeyGeneratorFactory: The value of hoodie.datasource.write.keygenerator.type is empty, use SIMPLE
22/02/12 19:07:30 INFO HoodieTableMetaClient: Initializing hdfs://namenode/user/admin/default/customer_order_relation/hudi as hoodie table hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:30 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:30 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:30 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:30 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:31 INFO DFSPathSelector: Using path selector org.apache.hudi.utilities.sources.helpers.DFSPathSelector
22/02/12 19:07:31 INFO HoodieDeltaStreamer: Delta Streamer running only single round
22/02/12 19:07:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:31 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:31 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:31 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
22/02/12 19:07:31 INFO DeltaSync: Checkpoint to resume from : Optional.empty
22/02/12 19:07:31 INFO DFSPathSelector: Root path => hdfs://namenode/user/admin/default/customer_order_relation/data source limit => 9223372036854775807
22/02/12 19:07:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/mozhenghua/j2ee_solution/project/plugins/tis-datax/tis-datax-hudi-plugin/spark-warehouse').
22/02/12 19:07:31 INFO SharedState: Warehouse path is 'file:/Users/mozhenghua/j2ee_solution/project/plugins/tis-datax/tis-datax-hudi-plugin/spark-warehouse'.
22/02/12 19:07:31 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
22/02/12 19:07:33 INFO FileSourceStrategy: Pruning directories with: 
22/02/12 19:07:33 INFO FileSourceStrategy: Post-Scan Filters: 
22/02/12 19:07:33 INFO FileSourceStrategy: Output Data Schema: struct<customerregister_id: string, waitingorder_id: string, kind: int, create_time: bigint, last_ver: bigint ... 3 more fields>
22/02/12 19:07:33 INFO FileSourceScanExec: Pushed Filters: 
22/02/12 19:07:33 INFO CodeGenerator: Code generated in 144.665147 ms
22/02/12 19:07:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 392.3 KB, free 365.9 MB)
22/02/12 19:07:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.7 KB, free 365.9 MB)
22/02/12 19:07:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.28.150:54588 (size: 24.7 KB, free: 366.3 MB)
22/02/12 19:07:34 INFO SparkContext: Created broadcast 0 from toRdd at HoodieSparkUtils.scala:152
22/02/12 19:07:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
22/02/12 19:07:34 INFO SparkContext: Starting job: isEmpty at DeltaSync.java:442
22/02/12 19:07:34 INFO DAGScheduler: Got job 0 (isEmpty at DeltaSync.java:442) with 1 output partitions
22/02/12 19:07:34 INFO DAGScheduler: Final stage: ResultStage 0 (isEmpty at DeltaSync.java:442)
22/02/12 19:07:34 INFO DAGScheduler: Parents of final stage: List()
22/02/12 19:07:34 INFO DAGScheduler: Missing parents: List()
22/02/12 19:07:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at HoodieSparkUtils.scala:153), which has no missing parents
22/02/12 19:07:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 15.5 KB, free 365.9 MB)
22/02/12 19:07:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KB, free 365.9 MB)
22/02/12 19:07:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.28.150:54588 (size: 8.1 KB, free: 366.3 MB)
22/02/12 19:07:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at mapPartitions at HoodieSparkUtils.scala:153) (first 15 tasks are for partitions Vector(0))
22/02/12 19:07:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
22/02/12 19:07:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.21.0.14, executor 0, partition 0, ANY, 8345 bytes)
22/02/12 19:07:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.0.14:44157 (size: 8.1 KB, free: 366.3 MB)
22/02/12 19:07:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.0.14:44157 (size: 24.7 KB, free: 366.3 MB)
22/02/12 19:07:38 INFO ContextCleaner: Cleaned accumulator 2
22/02/12 19:07:38 INFO ContextCleaner: Cleaned accumulator 3
22/02/12 19:07:38 INFO ContextCleaner: Cleaned accumulator 1
22/02/12 19:07:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3767 ms on 172.21.0.14 (executor 0) (1/1)
22/02/12 19:07:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
22/02/12 19:07:38 INFO DAGScheduler: ResultStage 0 (isEmpty at DeltaSync.java:442) finished in 3.815 s
22/02/12 19:07:38 INFO DAGScheduler: Job 0 finished: isEmpty at DeltaSync.java:442, took 3.871139 s
22/02/12 19:07:38 INFO DeltaSync: Setting up new Hoodie Write Client
22/02/12 19:07:38 INFO EmbeddedTimelineService: Starting Timeline service !!
22/02/12 19:07:38 INFO EmbeddedTimelineService: Overriding hostIp to (192.168.28.150) found in spark-conf. It was null
22/02/12 19:07:38 INFO FileSystemViewManager: Creating View Manager with storage type :EMBEDDED_KV_STORE
22/02/12 19:07:38 INFO FileSystemViewManager: Creating embedded rocks-db based Table View
22/02/12 19:07:38 INFO log: Logging initialized @12318ms to org.apache.hudi.org.eclipse.jetty.util.log.Slf4jLog
22/02/12 19:07:38 INFO Javalin: 
           __                      __ _
          / /____ _ _   __ ____ _ / /(_)____
     __  / // __ `/| | / // __ `// // // __ \
    / /_/ // /_/ / | |/ // /_/ // // // / / /
    \____/ \__,_/  |___/ \__,_//_//_//_/ /_/

        https://javalin.io/documentation

22/02/12 19:07:38 INFO Javalin: Starting Javalin ...
22/02/12 19:07:38 INFO Javalin: Listening on http://localhost:54594/
22/02/12 19:07:38 INFO Javalin: Javalin started in 103ms \o/
22/02/12 19:07:38 INFO TimelineService: Starting Timeline server on port :54594
22/02/12 19:07:38 INFO EmbeddedTimelineService: Started embedded timeline server at 192.168.28.150:54594
22/02/12 19:07:38 INFO AbstractHoodieClient: Timeline Server already running. Not restarting the service
22/02/12 19:07:38 INFO SparkContext: Starting job: isEmpty at DeltaSync.java:489
22/02/12 19:07:38 INFO DAGScheduler: Got job 1 (isEmpty at DeltaSync.java:489) with 1 output partitions
22/02/12 19:07:38 INFO DAGScheduler: Final stage: ResultStage 1 (isEmpty at DeltaSync.java:489)
22/02/12 19:07:38 INFO DAGScheduler: Parents of final stage: List()
22/02/12 19:07:38 INFO DAGScheduler: Missing parents: List()
22/02/12 19:07:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at DeltaSync.java:449), which has no missing parents
22/02/12 19:07:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 21.3 KB, free 365.8 MB)
22/02/12 19:07:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.0 KB, free 365.8 MB)
22/02/12 19:07:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.28.150:54588 (size: 11.0 KB, free: 366.3 MB)
22/02/12 19:07:38 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at DeltaSync.java:449) (first 15 tasks are for partitions Vector(0))
22/02/12 19:07:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
22/02/12 19:07:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 172.21.0.14, executor 0, partition 0, ANY, 8345 bytes)
22/02/12 19:07:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.0.14:44157 (size: 11.0 KB, free: 366.3 MB)
22/02/12 19:07:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 157 ms on 172.21.0.14 (executor 0) (1/1)
22/02/12 19:07:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
22/02/12 19:07:38 INFO DAGScheduler: ResultStage 1 (isEmpty at DeltaSync.java:489) finished in 0.165 s
22/02/12 19:07:38 INFO DAGScheduler: Job 1 finished: isEmpty at DeltaSync.java:489, took 0.166920 s
22/02/12 19:07:38 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:38 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:38 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:38 INFO HoodieTableMetaClient: Loading Active commit timeline for hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:38 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
22/02/12 19:07:38 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:38 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:38 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:38 INFO HoodieTableMetaClient: Loading Active commit timeline for hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:38 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
22/02/12 19:07:38 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
22/02/12 19:07:38 INFO FileSystemViewManager: Creating remote first table view
22/02/12 19:07:38 INFO AbstractHoodieWriteClient: Generate a new instant time: 20220212190738514 action: commit
22/02/12 19:07:38 INFO HoodieActiveTimeline: Creating a new instant [==>20220212190738514__commit__REQUESTED]
22/02/12 19:07:38 INFO DeltaSync: Starting commit  : 20220212190738514
22/02/12 19:07:38 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:38 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:38 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:38 INFO HoodieTableMetaClient: Loading Active commit timeline for hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20220212190738514__commit__REQUESTED]}
22/02/12 19:07:38 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
22/02/12 19:07:38 INFO FileSystemViewManager: Creating remote first table view
22/02/12 19:07:38 INFO FileSystemViewManager: Creating remote view for basePath hdfs://namenode/user/admin/default/customer_order_relation/hudi. Server=192.168.28.150:54594, Timeout=300
22/02/12 19:07:38 INFO FileSystemViewManager: Creating InMemory based view for basePath hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:38 INFO AbstractTableFileSystemView: Took 2 ms to read  0 instants, 0 replaced file groups
22/02/12 19:07:38 INFO ClusteringUtils: Found 0 files in pending clustering operations
22/02/12 19:07:38 INFO RemoteHoodieTableFileSystemView: Sending request : (http://192.168.28.150:54594/v1/hoodie/view/refresh/?basepath=hdfs%3A%2F%2Fnamenode%2Fuser%2Fadmin%2Fdefault%2Fcustomer_order_relation%2Fhudi&timelinehash=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855)
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 10
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 25
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 14
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 16
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 24
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 9
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 8
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 28
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 17
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 12
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 13
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 21
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 20
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 26
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 29
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 23
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 15
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 6
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 11
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 40
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 46
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 49
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 53
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 35
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 50
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 37
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 48
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 38
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 41
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 31
22/02/12 19:07:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.28.150:54588 in memory (size: 11.0 KB, free: 366.3 MB)
22/02/12 19:07:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.21.0.14:44157 in memory (size: 11.0 KB, free: 366.3 MB)
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 44
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 43
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 39
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 54
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 47
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 34
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 30
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 45
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 51
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 36
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 32
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 33
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 52
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 42
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 5
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 27
22/02/12 19:07:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.28.150:54588 in memory (size: 8.1 KB, free: 366.3 MB)
22/02/12 19:07:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.21.0.14:44157 in memory (size: 8.1 KB, free: 366.3 MB)
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 18
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 7
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 19
22/02/12 19:07:39 INFO ContextCleaner: Cleaned accumulator 22
22/02/12 19:07:39 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
22/02/12 19:07:39 INFO ClusteringUtils: Found 0 files in pending clustering operations
22/02/12 19:07:39 INFO AsyncCleanerService: Async auto cleaning is not enabled. Not running cleaner now
22/02/12 19:07:39 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:103
22/02/12 19:07:39 INFO DAGScheduler: Registering RDD 5 (mapToPair at SparkWriteHelper.java:63)
22/02/12 19:07:39 INFO DAGScheduler: Registering RDD 9 (countByKey at HoodieJavaPairRDD.java:103)
22/02/12 19:07:39 INFO DAGScheduler: Got job 2 (countByKey at HoodieJavaPairRDD.java:103) with 2 output partitions
22/02/12 19:07:39 INFO DAGScheduler: Final stage: ResultStage 4 (countByKey at HoodieJavaPairRDD.java:103)
22/02/12 19:07:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
22/02/12 19:07:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
22/02/12 19:07:39 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at mapToPair at SparkWriteHelper.java:63), which has no missing parents
22/02/12 19:07:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 23.4 KB, free 365.9 MB)
22/02/12 19:07:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.0 KB, free 365.9 MB)
22/02/12 19:07:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.28.150:54588 (size: 12.0 KB, free: 366.3 MB)
22/02/12 19:07:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at mapToPair at SparkWriteHelper.java:63) (first 15 tasks are for partitions Vector(0))
22/02/12 19:07:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
22/02/12 19:07:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 172.21.0.14, executor 0, partition 0, ANY, 8334 bytes)
22/02/12 19:07:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.21.0.14:44157 (size: 12.0 KB, free: 366.3 MB)
22/02/12 19:07:39 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 134 ms on 172.21.0.14 (executor 0) (1/1)
22/02/12 19:07:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
22/02/12 19:07:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at SparkWriteHelper.java:63) finished in 0.144 s
22/02/12 19:07:39 INFO DAGScheduler: looking for newly runnable stages
22/02/12 19:07:39 INFO DAGScheduler: running: Set()
22/02/12 19:07:39 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)
22/02/12 19:07:39 INFO DAGScheduler: failed: Set()
22/02/12 19:07:39 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at countByKey at HoodieJavaPairRDD.java:103), which has no missing parents
22/02/12 19:07:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.5 KB, free 365.9 MB)
22/02/12 19:07:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.4 KB, free 365.8 MB)
22/02/12 19:07:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.28.150:54588 (size: 3.4 KB, free: 366.3 MB)
22/02/12 19:07:39 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:39 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at countByKey at HoodieJavaPairRDD.java:103) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:39 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
22/02/12 19:07:39 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 172.21.0.14, executor 0, partition 0, NODE_LOCAL, 7655 bytes)
22/02/12 19:07:39 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, 172.21.0.14, executor 0, partition 1, NODE_LOCAL, 7655 bytes)
22/02/12 19:07:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.21.0.14:44157 (size: 3.4 KB, free: 366.3 MB)
22/02/12 19:07:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.28.201:52214
22/02/12 19:07:39 INFO BlockManagerInfo: Added rdd_7_0 in memory on 172.21.0.14:44157 (size: 1098.0 B, free: 366.3 MB)
22/02/12 19:07:39 INFO BlockManagerInfo: Added rdd_7_1 in memory on 172.21.0.14:44157 (size: 1881.0 B, free: 366.3 MB)
22/02/12 19:07:39 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 196 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 201 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
22/02/12 19:07:39 INFO DAGScheduler: ShuffleMapStage 3 (countByKey at HoodieJavaPairRDD.java:103) finished in 0.209 s
22/02/12 19:07:39 INFO DAGScheduler: looking for newly runnable stages
22/02/12 19:07:39 INFO DAGScheduler: running: Set()
22/02/12 19:07:39 INFO DAGScheduler: waiting: Set(ResultStage 4)
22/02/12 19:07:39 INFO DAGScheduler: failed: Set()
22/02/12 19:07:39 INFO DAGScheduler: Submitting ResultStage 4 (ShuffledRDD[10] at countByKey at HoodieJavaPairRDD.java:103), which has no missing parents
22/02/12 19:07:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.7 KB, free 365.8 MB)
22/02/12 19:07:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.8 MB)
22/02/12 19:07:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.28.150:54588 (size: 2.1 KB, free: 366.3 MB)
22/02/12 19:07:39 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:39 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[10] at countByKey at HoodieJavaPairRDD.java:103) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:39 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
22/02/12 19:07:39 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5, 172.21.0.14, executor 0, partition 0, NODE_LOCAL, 7666 bytes)
22/02/12 19:07:39 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 6, 172.21.0.14, executor 0, partition 1, NODE_LOCAL, 7666 bytes)
22/02/12 19:07:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.21.0.14:44157 (size: 2.1 KB, free: 366.3 MB)
22/02/12 19:07:39 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.28.201:52214
22/02/12 19:07:39 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 47 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:39 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 6) in 51 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:39 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
22/02/12 19:07:39 INFO DAGScheduler: ResultStage 4 (countByKey at HoodieJavaPairRDD.java:103) finished in 0.059 s
22/02/12 19:07:39 INFO DAGScheduler: Job 2 finished: countByKey at HoodieJavaPairRDD.java:103, took 0.443283 s
22/02/12 19:07:39 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:134
22/02/12 19:07:39 INFO DAGScheduler: Got job 3 (collect at HoodieSparkEngineContext.java:134) with 2 output partitions
22/02/12 19:07:39 INFO DAGScheduler: Final stage: ResultStage 5 (collect at HoodieSparkEngineContext.java:134)
22/02/12 19:07:39 INFO DAGScheduler: Parents of final stage: List()
22/02/12 19:07:39 INFO DAGScheduler: Missing parents: List()
22/02/12 19:07:39 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[12] at flatMap at HoodieSparkEngineContext.java:134), which has no missing parents
22/02/12 19:07:39 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 236.2 KB, free 365.6 MB)
22/02/12 19:07:39 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 83.9 KB, free 365.5 MB)
22/02/12 19:07:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.28.150:54588 (size: 83.9 KB, free: 366.2 MB)
22/02/12 19:07:39 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:39 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at flatMap at HoodieSparkEngineContext.java:134) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:39 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
22/02/12 19:07:39 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7729 bytes)
22/02/12 19:07:39 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 8, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7729 bytes)
22/02/12 19:07:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.21.0.14:44157 (size: 83.9 KB, free: 366.2 MB)
22/02/12 19:07:39 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 8) in 199 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:39 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 208 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:39 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
22/02/12 19:07:39 INFO DAGScheduler: ResultStage 5 (collect at HoodieSparkEngineContext.java:134) finished in 0.246 s
22/02/12 19:07:39 INFO DAGScheduler: Job 3 finished: collect at HoodieSparkEngineContext.java:134, took 0.247766 s
22/02/12 19:07:40 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:100
22/02/12 19:07:40 INFO DAGScheduler: Got job 4 (collect at HoodieSparkEngineContext.java:100) with 1 output partitions
22/02/12 19:07:40 INFO DAGScheduler: Final stage: ResultStage 6 (collect at HoodieSparkEngineContext.java:100)
22/02/12 19:07:40 INFO DAGScheduler: Parents of final stage: List()
22/02/12 19:07:40 INFO DAGScheduler: Missing parents: List()
22/02/12 19:07:40 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[14] at map at HoodieSparkEngineContext.java:100), which has no missing parents
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 236.0 KB, free 365.3 MB)
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 84.0 KB, free 365.2 MB)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.28.150:54588 (size: 84.0 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[14] at map at HoodieSparkEngineContext.java:100) (first 15 tasks are for partitions Vector(0))
22/02/12 19:07:40 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
22/02/12 19:07:40 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 9, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7725 bytes)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.21.0.14:44157 (size: 84.0 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 9) in 54 ms on 172.21.0.14 (executor 0) (1/1)
22/02/12 19:07:40 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
22/02/12 19:07:40 INFO DAGScheduler: ResultStage 6 (collect at HoodieSparkEngineContext.java:100) finished in 0.088 s
22/02/12 19:07:40 INFO DAGScheduler: Job 4 finished: collect at HoodieSparkEngineContext.java:100, took 0.090276 s
22/02/12 19:07:40 INFO SparkContext: Starting job: countByKey at SparkHoodieBloomIndexHelper.java:113
22/02/12 19:07:40 INFO DAGScheduler: Registering RDD 19 (countByKey at SparkHoodieBloomIndexHelper.java:113)
22/02/12 19:07:40 INFO DAGScheduler: Got job 5 (countByKey at SparkHoodieBloomIndexHelper.java:113) with 2 output partitions
22/02/12 19:07:40 INFO DAGScheduler: Final stage: ResultStage 9 (countByKey at SparkHoodieBloomIndexHelper.java:113)
22/02/12 19:07:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
22/02/12 19:07:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)
22/02/12 19:07:40 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[19] at countByKey at SparkHoodieBloomIndexHelper.java:113), which has no missing parents
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 8.9 KB, free 365.2 MB)
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.5 KB, free 365.2 MB)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.28.150:54588 (size: 4.5 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[19] at countByKey at SparkHoodieBloomIndexHelper.java:113) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:40 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks
22/02/12 19:07:40 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7655 bytes)
22/02/12 19:07:40 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 11, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7655 bytes)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.21.0.14:44157 (size: 4.5 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 11) in 51 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 52 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:40 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
22/02/12 19:07:40 INFO DAGScheduler: ShuffleMapStage 8 (countByKey at SparkHoodieBloomIndexHelper.java:113) finished in 0.060 s
22/02/12 19:07:40 INFO DAGScheduler: looking for newly runnable stages
22/02/12 19:07:40 INFO DAGScheduler: running: Set()
22/02/12 19:07:40 INFO DAGScheduler: waiting: Set(ResultStage 9)
22/02/12 19:07:40 INFO DAGScheduler: failed: Set()
22/02/12 19:07:40 INFO DAGScheduler: Submitting ResultStage 9 (ShuffledRDD[20] at countByKey at SparkHoodieBloomIndexHelper.java:113), which has no missing parents
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 3.7 KB, free 365.2 MB)
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.2 MB)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.28.150:54588 (size: 2.1 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (ShuffledRDD[20] at countByKey at SparkHoodieBloomIndexHelper.java:113) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:40 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks
22/02/12 19:07:40 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 12, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7666 bytes)
22/02/12 19:07:40 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 13, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7666 bytes)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.21.0.14:44157 (size: 2.1 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.28.201:52214
22/02/12 19:07:40 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 12) in 42 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 13) in 42 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:40 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
22/02/12 19:07:40 INFO DAGScheduler: ResultStage 9 (countByKey at SparkHoodieBloomIndexHelper.java:113) finished in 0.051 s
22/02/12 19:07:40 INFO DAGScheduler: Job 5 finished: countByKey at SparkHoodieBloomIndexHelper.java:113, took 0.115664 s
22/02/12 19:07:40 INFO SparkHoodieBloomIndexHelper: InputParallelism: ${2}, IndexParallelism: ${0}
22/02/12 19:07:40 INFO BucketizedBloomCheckPartitioner: TotalBuckets 0, min_buckets/partition 1
22/02/12 19:07:40 INFO MapPartitionsRDD: Removing RDD 7 from persistence list
22/02/12 19:07:40 INFO BlockManager: Removing RDD 7
22/02/12 19:07:40 INFO MapPartitionsRDD: Removing RDD 27 from persistence list
22/02/12 19:07:40 INFO BlockManager: Removing RDD 27
22/02/12 19:07:40 INFO SparkContext: Starting job: countByKey at BaseSparkCommitActionExecutor.java:191
22/02/12 19:07:40 INFO DAGScheduler: Registering RDD 21 (mapToPair at SparkHoodieBloomIndexHelper.java:85)
22/02/12 19:07:40 INFO DAGScheduler: Registering RDD 28 (mapToPair at HoodieJavaRDD.java:122)
22/02/12 19:07:40 INFO DAGScheduler: Registering RDD 27 (flatMapToPair at SparkHoodieBloomIndexHelper.java:93)
22/02/12 19:07:40 INFO DAGScheduler: Registering RDD 37 (countByKey at BaseSparkCommitActionExecutor.java:191)
22/02/12 19:07:40 INFO DAGScheduler: Got job 6 (countByKey at BaseSparkCommitActionExecutor.java:191) with 2 output partitions
22/02/12 19:07:40 INFO DAGScheduler: Final stage: ResultStage 15 (countByKey at BaseSparkCommitActionExecutor.java:191)
22/02/12 19:07:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
22/02/12 19:07:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 14)
22/02/12 19:07:40 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[28] at mapToPair at HoodieJavaRDD.java:122), which has no missing parents
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 6.3 KB, free 365.2 MB)
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.4 KB, free 365.2 MB)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.28.150:54588 (size: 3.4 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[28] at mapToPair at HoodieJavaRDD.java:122) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:40 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
22/02/12 19:07:40 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 14, 172.21.0.14, executor 0, partition 0, NODE_LOCAL, 7655 bytes)
22/02/12 19:07:40 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 15, 172.21.0.14, executor 0, partition 1, NODE_LOCAL, 7655 bytes)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.21.0.14:44157 (size: 3.4 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.28.201:52214
22/02/12 19:07:40 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 14) in 68 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 15) in 69 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:40 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
22/02/12 19:07:40 INFO DAGScheduler: ShuffleMapStage 12 (mapToPair at HoodieJavaRDD.java:122) finished in 0.074 s
22/02/12 19:07:40 INFO DAGScheduler: looking for newly runnable stages
22/02/12 19:07:40 INFO DAGScheduler: running: Set()
22/02/12 19:07:40 INFO DAGScheduler: waiting: Set(ResultStage 15, ShuffleMapStage 14)
22/02/12 19:07:40 INFO DAGScheduler: failed: Set()
22/02/12 19:07:40 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[37] at countByKey at BaseSparkCommitActionExecutor.java:191), which has no missing parents
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 7.6 KB, free 365.2 MB)
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.0 KB, free 365.2 MB)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.28.150:54588 (size: 4.0 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[37] at countByKey at BaseSparkCommitActionExecutor.java:191) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:40 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks
22/02/12 19:07:40 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 16, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7718 bytes)
22/02/12 19:07:40 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 17, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7718 bytes)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.21.0.14:44157 (size: 4.0 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.28.201:52214
22/02/12 19:07:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 192.168.28.201:52214
22/02/12 19:07:40 INFO BlockManagerInfo: Added rdd_35_1 in memory on 172.21.0.14:44157 (size: 1881.0 B, free: 366.1 MB)
22/02/12 19:07:40 INFO BlockManagerInfo: Added rdd_35_0 in memory on 172.21.0.14:44157 (size: 1098.0 B, free: 366.1 MB)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 17) in 117 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 16) in 124 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:40 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
22/02/12 19:07:40 INFO DAGScheduler: ShuffleMapStage 14 (countByKey at BaseSparkCommitActionExecutor.java:191) finished in 0.133 s
22/02/12 19:07:40 INFO DAGScheduler: looking for newly runnable stages
22/02/12 19:07:40 INFO DAGScheduler: running: Set()
22/02/12 19:07:40 INFO DAGScheduler: waiting: Set(ResultStage 15)
22/02/12 19:07:40 INFO DAGScheduler: failed: Set()
22/02/12 19:07:40 INFO DAGScheduler: Submitting ResultStage 15 (ShuffledRDD[38] at countByKey at BaseSparkCommitActionExecutor.java:191), which has no missing parents
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 3.7 KB, free 365.2 MB)
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.1 KB, free 365.2 MB)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.28.150:54588 (size: 2.1 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 15 (ShuffledRDD[38] at countByKey at BaseSparkCommitActionExecutor.java:191) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:40 INFO TaskSchedulerImpl: Adding task set 15.0 with 2 tasks
22/02/12 19:07:40 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 18, 172.21.0.14, executor 0, partition 1, NODE_LOCAL, 7666 bytes)
22/02/12 19:07:40 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 19, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7666 bytes)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.21.0.14:44157 (size: 2.1 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 192.168.28.201:52214
22/02/12 19:07:40 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 19) in 43 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 18) in 46 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:40 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
22/02/12 19:07:40 INFO DAGScheduler: ResultStage 15 (countByKey at BaseSparkCommitActionExecutor.java:191) finished in 0.054 s
22/02/12 19:07:40 INFO DAGScheduler: Job 6 finished: countByKey at BaseSparkCommitActionExecutor.java:191, took 0.268504 s
22/02/12 19:07:40 INFO BaseSparkCommitActionExecutor: Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=30, numUpdates=0}, partitionStat={1=WorkloadStat {numInserts=10, numUpdates=0}, 2=WorkloadStat {numInserts=20, numUpdates=0}}, operationType=UPSERT}
22/02/12 19:07:40 INFO HoodieActiveTimeline: Checking for file exists ?hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/20220212190738514.commit.requested
22/02/12 19:07:40 INFO HoodieActiveTimeline: Create new file for toInstant ?hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/20220212190738514.inflight
22/02/12 19:07:40 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
22/02/12 19:07:40 INFO ClusteringUtils: Found 0 files in pending clustering operations
22/02/12 19:07:40 INFO SparkContext: Starting job: collect at SparkRejectUpdateStrategy.java:52
22/02/12 19:07:40 INFO DAGScheduler: Registering RDD 41 (distinct at SparkRejectUpdateStrategy.java:52)
22/02/12 19:07:40 INFO DAGScheduler: Got job 7 (collect at SparkRejectUpdateStrategy.java:52) with 2 output partitions
22/02/12 19:07:40 INFO DAGScheduler: Final stage: ResultStage 21 (collect at SparkRejectUpdateStrategy.java:52)
22/02/12 19:07:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
22/02/12 19:07:40 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)
22/02/12 19:07:40 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[41] at distinct at SparkRejectUpdateStrategy.java:52), which has no missing parents
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 7.3 KB, free 365.2 MB)
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.8 KB, free 365.2 MB)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.28.150:54588 (size: 3.8 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[41] at distinct at SparkRejectUpdateStrategy.java:52) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:40 INFO TaskSchedulerImpl: Adding task set 20.0 with 2 tasks
22/02/12 19:07:40 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7718 bytes)
22/02/12 19:07:40 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 21, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7718 bytes)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.21.0.14:44157 (size: 3.8 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 21) in 38 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 41 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:40 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
22/02/12 19:07:40 INFO DAGScheduler: ShuffleMapStage 20 (distinct at SparkRejectUpdateStrategy.java:52) finished in 0.048 s
22/02/12 19:07:40 INFO DAGScheduler: looking for newly runnable stages
22/02/12 19:07:40 INFO DAGScheduler: running: Set()
22/02/12 19:07:40 INFO DAGScheduler: waiting: Set(ResultStage 21)
22/02/12 19:07:40 INFO DAGScheduler: failed: Set()
22/02/12 19:07:40 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[43] at distinct at SparkRejectUpdateStrategy.java:52), which has no missing parents
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 4.2 KB, free 365.2 MB)
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.4 KB, free 365.2 MB)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.28.150:54588 (size: 2.4 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at distinct at SparkRejectUpdateStrategy.java:52) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:40 INFO TaskSchedulerImpl: Adding task set 21.0 with 2 tasks
22/02/12 19:07:40 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 22, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7666 bytes)
22/02/12 19:07:40 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 23, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7666 bytes)
22/02/12 19:07:40 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.21.0.14:44157 (size: 2.4 KB, free: 366.1 MB)
22/02/12 19:07:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 192.168.28.201:52214
22/02/12 19:07:40 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 22) in 35 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:40 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 23) in 36 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:40 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
22/02/12 19:07:40 INFO DAGScheduler: ResultStage 21 (collect at SparkRejectUpdateStrategy.java:52) finished in 0.043 s
22/02/12 19:07:40 INFO DAGScheduler: Job 7 finished: collect at SparkRejectUpdateStrategy.java:52, took 0.095930 s
22/02/12 19:07:40 INFO UpsertPartitioner: AvgRecordSize => 1024
22/02/12 19:07:40 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:256
22/02/12 19:07:40 INFO DAGScheduler: Got job 8 (collectAsMap at UpsertPartitioner.java:256) with 2 output partitions
22/02/12 19:07:40 INFO DAGScheduler: Final stage: ResultStage 22 (collectAsMap at UpsertPartitioner.java:256)
22/02/12 19:07:40 INFO DAGScheduler: Parents of final stage: List()
22/02/12 19:07:40 INFO DAGScheduler: Missing parents: List()
22/02/12 19:07:40 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[45] at mapToPair at UpsertPartitioner.java:255), which has no missing parents
22/02/12 19:07:40 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 236.8 KB, free 364.9 MB)
22/02/12 19:07:41 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 84.2 KB, free 364.8 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.28.150:54588 (size: 84.2 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at mapToPair at UpsertPartitioner.java:255) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:41 INFO TaskSchedulerImpl: Adding task set 22.0 with 2 tasks
22/02/12 19:07:41 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 24, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7729 bytes)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 331
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 148
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 290
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 227
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 313
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 248
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 89
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 206
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 334
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 105
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 262
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 92
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 59
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 265
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 285
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 197
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 272
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 324
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 320
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 199
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 152
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 235
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 252
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 201
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 162
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 351
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 333
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 95
22/02/12 19:07:41 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 25, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7729 bytes)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 174
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 305
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 103
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 127
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 268
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 118
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 220
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 338
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 123
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 306
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 352
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 223
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 115
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 208
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 322
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 61
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 178
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 88
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 276
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 295
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 112
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 318
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 343
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 69
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 317
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 91
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 94
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 145
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 163
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 286
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 300
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 323
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 203
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 160
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 82
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 260
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 253
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 74
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 215
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 258
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 342
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 77
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 85
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 140
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 180
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 233
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 232
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 304
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 134
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 135
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 154
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 250
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 348
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 245
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 309
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 177
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 349
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 231
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 78
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 263
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 267
22/02/12 19:07:41 INFO ContextCleaner: Cleaned shuffle 7
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 181
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 150
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 282
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 315
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 93
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 139
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 335
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 240
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 212
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 79
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 171
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 256
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 102
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 242
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 71
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 205
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 58
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 111
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 347
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.28.150:54588 in memory (size: 12.0 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.21.0.14:44157 in memory (size: 12.0 KB, free: 366.1 MB)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 251
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 141
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 229
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 101
22/02/12 19:07:41 INFO ContextCleaner: Cleaned shuffle 5
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 114
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.28.150:54588 in memory (size: 2.1 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.21.0.14:44157 (size: 84.2 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.21.0.14:44157 in memory (size: 2.1 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 25) in 54 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:41 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 24) in 57 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:41 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
22/02/12 19:07:41 INFO DAGScheduler: ResultStage 22 (collectAsMap at UpsertPartitioner.java:256) finished in 0.102 s
22/02/12 19:07:41 INFO DAGScheduler: Job 8 finished: collectAsMap at UpsertPartitioner.java:256, took 0.103726 s
22/02/12 19:07:41 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 176
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 249
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 63
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 97
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 159
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 264
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 130
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 261
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 168
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 138
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 284
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 147
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 329
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 60
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 275
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.28.150:54588 in memory (size: 3.4 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO ClusteringUtils: Found 0 files in pending clustering operations
22/02/12 19:07:41 INFO UpsertPartitioner: For partitionPath : 1 Small Files => []
22/02/12 19:07:41 INFO UpsertPartitioner: After small file assignment: unassignedInserts => 10, totalInsertBuckets => 1, recordsPerBucket => 122880
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.21.0.14:44157 in memory (size: 3.4 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO UpsertPartitioner: Total insert buckets for partition path 1 => [(InsertBucket {bucketNumber=0, weight=1.0},1.0)]
22/02/12 19:07:41 INFO UpsertPartitioner: For partitionPath : 2 Small Files => []
22/02/12 19:07:41 INFO UpsertPartitioner: After small file assignment: unassignedInserts => 20, totalInsertBuckets => 1, recordsPerBucket => 122880
22/02/12 19:07:41 INFO UpsertPartitioner: Total insert buckets for partition path 2 => [(InsertBucket {bucketNumber=1, weight=1.0},1.0)]
22/02/12 19:07:41 INFO UpsertPartitioner: Total Buckets :2, buckets info => {0=BucketInfo {bucketType=INSERT, fileIdPrefix=290f9c8b-adfa-42f1-9fa2-118d44c4a605, partitionPath=1}, 1=BucketInfo {bucketType=INSERT, fileIdPrefix=a5db7715-57ed-4b2e-b329-80608cb0a1fe, partitionPath=2}}, 
Partition to insert buckets => {1=[(InsertBucket {bucketNumber=0, weight=1.0},1.0)], 2=[(InsertBucket {bucketNumber=1, weight=1.0},1.0)]}, 
UpdateLocations mapped to buckets =>{}
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 128
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 64
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.28.150:54588 in memory (size: 2.1 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.21.0.14:44157 in memory (size: 2.1 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 279
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 76
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 113
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 107
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 327
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 151
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 186
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 296
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 183
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 75
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.28.150:54588 in memory (size: 84.0 KB, free: 366.1 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.21.0.14:44157 in memory (size: 84.0 KB, free: 366.1 MB)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 219
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 153
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 200
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 297
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 274
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 172
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 257
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 316
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 119
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 161
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 314
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 291
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 307
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.28.150:54588 in memory (size: 83.9 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.21.0.14:44157 in memory (size: 83.9 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 66
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 218
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 294
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 179
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 330
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 109
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 175
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 182
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 244
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 204
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 170
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 350
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 209
22/02/12 19:07:41 INFO ContextCleaner: Cleaned shuffle 0
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 255
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 185
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.28.150:54588 in memory (size: 2.1 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.21.0.14:44157 in memory (size: 2.1 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 236
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 277
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 228
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 310
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 117
22/02/12 19:07:41 INFO ContextCleaner: Cleaned shuffle 2
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 191
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 110
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 225
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 131
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 142
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 354
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 144
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 195
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 246
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 266
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 328
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 321
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 280
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 173
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 319
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 158
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 167
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 196
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 287
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 65
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 108
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 281
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 216
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 247
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 283
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 187
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 157
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 137
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 193
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 189
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 241
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 122
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 156
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 73
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 116
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 293
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 336
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 87
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 292
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 81
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 169
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 301
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 133
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 259
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 270
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 340
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 100
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 210
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 146
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 214
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 302
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.28.150:54588 in memory (size: 4.0 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.21.0.14:44157 in memory (size: 4.0 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 165
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 62
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 198
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 99
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 224
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 312
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 55
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 70
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 288
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 90
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 98
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 121
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 207
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 353
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 269
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 344
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 143
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 308
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.28.150:54588 in memory (size: 4.5 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO BaseSparkCommitActionExecutor: no validators configured.
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.21.0.14:44157 in memory (size: 4.5 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO BaseCommitActionExecutor: Auto commit disabled for 20220212190738514
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 96
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 217
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 222
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 230
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.28.150:54588 in memory (size: 3.8 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.21.0.14:44157 in memory (size: 3.8 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 166
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 243
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 238
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 125
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 126
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 104
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 271
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 106
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 129
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 124
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 237
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 202
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 72
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 339
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 132
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 84
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 149
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 188
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 67
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 184
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 192
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 345
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 68
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 337
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 83
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 80
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 289
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 211
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 136
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 341
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.28.150:54588 in memory (size: 3.4 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.21.0.14:44157 in memory (size: 3.4 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 190
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 273
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 254
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 155
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 303
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 326
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 278
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 325
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 120
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 221
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 213
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 57
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 164
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 299
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 311
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.28.150:54588 in memory (size: 2.4 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.21.0.14:44157 in memory (size: 2.4 KB, free: 366.2 MB)
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 239
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 346
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 298
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 226
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 234
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 332
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 194
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 56
22/02/12 19:07:41 INFO ContextCleaner: Cleaned accumulator 86
22/02/12 19:07:41 INFO SparkContext: Starting job: sum at DeltaSync.java:516
22/02/12 19:07:41 INFO DAGScheduler: Registering RDD 46 (mapToPair at BaseSparkCommitActionExecutor.java:225)
22/02/12 19:07:41 INFO DAGScheduler: Got job 9 (sum at DeltaSync.java:516) with 2 output partitions
22/02/12 19:07:41 INFO DAGScheduler: Final stage: ResultStage 28 (sum at DeltaSync.java:516)
22/02/12 19:07:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
22/02/12 19:07:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 27)
22/02/12 19:07:41 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[46] at mapToPair at BaseSparkCommitActionExecutor.java:225), which has no missing parents
22/02/12 19:07:41 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 241.4 KB, free 365.3 MB)
22/02/12 19:07:41 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 86.2 KB, free 365.3 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.28.150:54588 (size: 86.2 KB, free: 366.1 MB)
22/02/12 19:07:41 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[46] at mapToPair at BaseSparkCommitActionExecutor.java:225) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:41 INFO TaskSchedulerImpl: Adding task set 27.0 with 2 tasks
22/02/12 19:07:41 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 26, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7718 bytes)
22/02/12 19:07:41 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 27, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7718 bytes)
22/02/12 19:07:41 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.21.0.14:44157 (size: 86.2 KB, free: 366.1 MB)
22/02/12 19:07:41 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 27) in 59 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:41 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 26) in 66 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:41 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
22/02/12 19:07:41 INFO DAGScheduler: ShuffleMapStage 27 (mapToPair at BaseSparkCommitActionExecutor.java:225) finished in 0.102 s
22/02/12 19:07:41 INFO DAGScheduler: looking for newly runnable stages
22/02/12 19:07:41 INFO DAGScheduler: running: Set()
22/02/12 19:07:41 INFO DAGScheduler: waiting: Set(ResultStage 28)
22/02/12 19:07:41 INFO DAGScheduler: failed: Set()
22/02/12 19:07:41 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[51] at mapToDouble at DeltaSync.java:516), which has no missing parents
22/02/12 19:07:41 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 330.5 KB, free 364.9 MB)
22/02/12 19:07:41 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 121.9 KB, free 364.8 MB)
22/02/12 19:07:41 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.28.150:54588 (size: 121.9 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 28 (MapPartitionsRDD[51] at mapToDouble at DeltaSync.java:516) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:41 INFO TaskSchedulerImpl: Adding task set 28.0 with 2 tasks
22/02/12 19:07:41 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28, 172.21.0.14, executor 0, partition 0, NODE_LOCAL, 7666 bytes)
22/02/12 19:07:41 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 29, 172.21.0.14, executor 0, partition 1, NODE_LOCAL, 7666 bytes)
22/02/12 19:07:41 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.21.0.14:44157 (size: 121.9 KB, free: 366.0 MB)
22/02/12 19:07:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 192.168.28.201:52214
22/02/12 19:07:42 INFO BlockManagerInfo: Added rdd_50_1 in memory on 172.21.0.14:44157 (size: 355.0 B, free: 366.0 MB)
22/02/12 19:07:42 INFO BlockManagerInfo: Added rdd_50_0 in memory on 172.21.0.14:44157 (size: 355.0 B, free: 366.0 MB)
22/02/12 19:07:42 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 759 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:42 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 29) in 759 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:42 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
22/02/12 19:07:42 INFO DAGScheduler: ResultStage 28 (sum at DeltaSync.java:516) finished in 0.812 s
22/02/12 19:07:42 INFO DAGScheduler: Job 9 finished: sum at DeltaSync.java:516, took 0.918983 s
22/02/12 19:07:42 INFO SparkContext: Starting job: sum at DeltaSync.java:517
22/02/12 19:07:42 INFO DAGScheduler: Got job 10 (sum at DeltaSync.java:517) with 2 output partitions
22/02/12 19:07:42 INFO DAGScheduler: Final stage: ResultStage 34 (sum at DeltaSync.java:517)
22/02/12 19:07:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
22/02/12 19:07:42 INFO DAGScheduler: Missing parents: List()
22/02/12 19:07:42 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[53] at mapToDouble at DeltaSync.java:517), which has no missing parents
22/02/12 19:07:42 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 330.5 KB, free 364.5 MB)
22/02/12 19:07:42 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 121.9 KB, free 364.4 MB)
22/02/12 19:07:42 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.28.150:54588 (size: 121.9 KB, free: 365.9 MB)
22/02/12 19:07:42 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 34 (MapPartitionsRDD[53] at mapToDouble at DeltaSync.java:517) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:42 INFO TaskSchedulerImpl: Adding task set 34.0 with 2 tasks
22/02/12 19:07:42 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 30, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7666 bytes)
22/02/12 19:07:42 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 31, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7666 bytes)
22/02/12 19:07:42 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.21.0.14:44157 (size: 121.9 KB, free: 365.9 MB)
22/02/12 19:07:42 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 30) in 53 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:42 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 31) in 54 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:42 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
22/02/12 19:07:42 INFO DAGScheduler: ResultStage 34 (sum at DeltaSync.java:517) finished in 0.096 s
22/02/12 19:07:42 INFO DAGScheduler: Job 10 finished: sum at DeltaSync.java:517, took 0.098722 s
22/02/12 19:07:42 INFO SparkContext: Starting job: collect at SparkRDDWriteClient.java:123
22/02/12 19:07:42 INFO DAGScheduler: Got job 11 (collect at SparkRDDWriteClient.java:123) with 2 output partitions
22/02/12 19:07:42 INFO DAGScheduler: Final stage: ResultStage 40 (collect at SparkRDDWriteClient.java:123)
22/02/12 19:07:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)
22/02/12 19:07:42 INFO DAGScheduler: Missing parents: List()
22/02/12 19:07:42 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[55] at map at SparkRDDWriteClient.java:123), which has no missing parents
22/02/12 19:07:42 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 330.5 KB, free 364.1 MB)
22/02/12 19:07:42 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 121.9 KB, free 363.9 MB)
22/02/12 19:07:42 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.28.150:54588 (size: 121.9 KB, free: 365.8 MB)
22/02/12 19:07:42 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 40 (MapPartitionsRDD[55] at map at SparkRDDWriteClient.java:123) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:42 INFO TaskSchedulerImpl: Adding task set 40.0 with 2 tasks
22/02/12 19:07:42 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 32, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7666 bytes)
22/02/12 19:07:42 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 33, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7666 bytes)
22/02/12 19:07:42 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.21.0.14:44157 (size: 121.9 KB, free: 365.7 MB)
22/02/12 19:07:42 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 33) in 68 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:42 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 32) in 68 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:42 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
22/02/12 19:07:42 INFO DAGScheduler: ResultStage 40 (collect at SparkRDDWriteClient.java:123) finished in 0.112 s
22/02/12 19:07:42 INFO DAGScheduler: Job 11 finished: collect at SparkRDDWriteClient.java:123, took 0.115254 s
22/02/12 19:07:42 INFO AbstractHoodieWriteClient: Committing 20220212190738514 action commit
22/02/12 19:07:42 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:42 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieTableMetaClient: Loading Active commit timeline for hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20220212190738514__commit__INFLIGHT]}
22/02/12 19:07:42 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
22/02/12 19:07:42 INFO FileSystemViewManager: Creating remote first table view
22/02/12 19:07:42 INFO CommitUtils: Creating  metadata for UPSERT numWriteStats:2numReplaceFileIds:0
22/02/12 19:07:42 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:42 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieTableMetaClient: Loading Active commit timeline for hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20220212190738514__commit__INFLIGHT]}
22/02/12 19:07:42 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
22/02/12 19:07:42 INFO FileSystemViewManager: Creating remote first table view
22/02/12 19:07:42 INFO AbstractHoodieWriteClient: Committing 20220212190738514 action commit
22/02/12 19:07:42 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:134
22/02/12 19:07:42 INFO DAGScheduler: Got job 12 (collect at HoodieSparkEngineContext.java:134) with 2 output partitions
22/02/12 19:07:42 INFO DAGScheduler: Final stage: ResultStage 41 (collect at HoodieSparkEngineContext.java:134)
22/02/12 19:07:42 INFO DAGScheduler: Parents of final stage: List()
22/02/12 19:07:42 INFO DAGScheduler: Missing parents: List()
22/02/12 19:07:42 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[57] at flatMap at HoodieSparkEngineContext.java:134), which has no missing parents
22/02/12 19:07:42 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 72.7 KB, free 363.9 MB)
22/02/12 19:07:42 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 26.1 KB, free 363.8 MB)
22/02/12 19:07:42 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.28.150:54588 (size: 26.1 KB, free: 365.7 MB)
22/02/12 19:07:42 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 41 (MapPartitionsRDD[57] at flatMap at HoodieSparkEngineContext.java:134) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:42 INFO TaskSchedulerImpl: Adding task set 41.0 with 2 tasks
22/02/12 19:07:42 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 34, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7826 bytes)
22/02/12 19:07:42 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 35, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7826 bytes)
22/02/12 19:07:42 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.21.0.14:44157 (size: 26.1 KB, free: 365.7 MB)
22/02/12 19:07:42 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 35) in 41 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:42 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 34) in 57 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:42 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
22/02/12 19:07:42 INFO DAGScheduler: ResultStage 41 (collect at HoodieSparkEngineContext.java:134) finished in 0.076 s
22/02/12 19:07:42 INFO DAGScheduler: Job 12 finished: collect at HoodieSparkEngineContext.java:134, took 0.077304 s
22/02/12 19:07:42 INFO HoodieActiveTimeline: Marking instant complete [==>20220212190738514__commit__INFLIGHT]
22/02/12 19:07:42 INFO HoodieActiveTimeline: Checking for file exists ?hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/20220212190738514.inflight
22/02/12 19:07:42 INFO HoodieActiveTimeline: Create new file for toInstant ?hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/20220212190738514.commit
22/02/12 19:07:42 INFO HoodieActiveTimeline: Completed [==>20220212190738514__commit__INFLIGHT]
22/02/12 19:07:42 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:148
22/02/12 19:07:42 INFO DAGScheduler: Got job 13 (collectAsMap at HoodieSparkEngineContext.java:148) with 2 output partitions
22/02/12 19:07:42 INFO DAGScheduler: Final stage: ResultStage 42 (collectAsMap at HoodieSparkEngineContext.java:148)
22/02/12 19:07:42 INFO DAGScheduler: Parents of final stage: List()
22/02/12 19:07:42 INFO DAGScheduler: Missing parents: List()
22/02/12 19:07:42 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[59] at mapToPair at HoodieSparkEngineContext.java:145), which has no missing parents
22/02/12 19:07:42 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 72.9 KB, free 363.8 MB)
22/02/12 19:07:42 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 26.2 KB, free 363.7 MB)
22/02/12 19:07:42 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.28.150:54588 (size: 26.2 KB, free: 365.7 MB)
22/02/12 19:07:42 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161
22/02/12 19:07:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 42 (MapPartitionsRDD[59] at mapToPair at HoodieSparkEngineContext.java:145) (first 15 tasks are for partitions Vector(0, 1))
22/02/12 19:07:42 INFO TaskSchedulerImpl: Adding task set 42.0 with 2 tasks
22/02/12 19:07:42 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 36, 172.21.0.14, executor 0, partition 0, PROCESS_LOCAL, 7826 bytes)
22/02/12 19:07:42 INFO TaskSetManager: Starting task 1.0 in stage 42.0 (TID 37, 172.21.0.14, executor 0, partition 1, PROCESS_LOCAL, 7826 bytes)
22/02/12 19:07:42 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.21.0.14:44157 (size: 26.2 KB, free: 365.7 MB)
22/02/12 19:07:42 INFO TaskSetManager: Finished task 1.0 in stage 42.0 (TID 37) in 39 ms on 172.21.0.14 (executor 0) (1/2)
22/02/12 19:07:42 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 36) in 42 ms on 172.21.0.14 (executor 0) (2/2)
22/02/12 19:07:42 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
22/02/12 19:07:42 INFO DAGScheduler: ResultStage 42 (collectAsMap at HoodieSparkEngineContext.java:148) finished in 0.059 s
22/02/12 19:07:42 INFO DAGScheduler: Job 13 finished: collectAsMap at HoodieSparkEngineContext.java:148, took 0.060033 s
22/02/12 19:07:42 INFO FSUtils: Removed directory at hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/.temp/20220212190738514
22/02/12 19:07:42 INFO AbstractHoodieWriteClient: Auto cleaning is enabled. Running cleaner now
22/02/12 19:07:42 INFO AbstractHoodieWriteClient: Scheduling cleaning at instant time :20220212190742776
22/02/12 19:07:42 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:42 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieTableMetaClient: Loading Active commit timeline for hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20220212190738514__commit__COMPLETED]}
22/02/12 19:07:42 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
22/02/12 19:07:42 INFO FileSystemViewManager: Creating remote first table view
22/02/12 19:07:42 INFO FileSystemViewManager: Creating remote view for basePath hdfs://namenode/user/admin/default/customer_order_relation/hudi. Server=192.168.28.150:54594, Timeout=300
22/02/12 19:07:42 INFO FileSystemViewManager: Creating InMemory based view for basePath hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
22/02/12 19:07:42 INFO ClusteringUtils: Found 0 files in pending clustering operations
22/02/12 19:07:42 INFO RemoteHoodieTableFileSystemView: Sending request : (http://192.168.28.150:54594/v1/hoodie/view/compactions/pending/?basepath=hdfs%3A%2F%2Fnamenode%2Fuser%2Fadmin%2Fdefault%2Fcustomer_order_relation%2Fhudi&lastinstantts=20220212190738514&timelinehash=4c9073abcb8d7004cc47014aeb012eb3d3e4c8460626e977997ba9eade59a5e6)
22/02/12 19:07:42 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:42 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:42 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20220212190738514__commit__COMPLETED]}
22/02/12 19:07:42 INFO RocksDBDAO: DELETING RocksDB persisted at /tmp/hoodie_timeline_rocksdb/hdfs:__namenode_user_admin_default_customer_order_relation_hudi/35b814f2-1633-4829-8b07-a13ceea874eb
22/02/12 19:07:42 INFO RocksDBDAO: No column family found. Loading default
22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/db_impl_open.cc:230] Creating manifest 1 

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/version_set.cc:3406] Recovering from manifest file: MANIFEST-000001

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/column_family.cc:475] --------------- Options for column family [default]:

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/version_set.cc:3610] Recovered from manifest file:/tmp/hoodie_timeline_rocksdb/hdfs:__namenode_user_admin_default_customer_order_relation_hudi/35b814f2-1633-4829-8b07-a13ceea874eb/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/version_set.cc:3618] Column family [default] (ID 0), log number is 0

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/db_impl_open.cc:1287] DB pointer 0x7fa6671b5e00
22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/version_set.cc:2936] Creating manifest 6

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/column_family.cc:475] --------------- Options for column family [hudi_view_hdfs:__namenode_user_admin_default_customer_order_relation_hudi]:

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/db_impl.cc:1546] Created column family [hudi_view_hdfs:__namenode_user_admin_default_customer_order_relation_hudi] (ID 1)
22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/column_family.cc:475] --------------- Options for column family [hudi_pending_compaction_hdfs:__namenode_user_admin_default_customer_order_relation_hudi]:

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/db_impl.cc:1546] Created column family [hudi_pending_compaction_hdfs:__namenode_user_admin_default_customer_order_relation_hudi] (ID 2)
22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/column_family.cc:475] --------------- Options for column family [hudi_bootstrap_basefile_hdfs:__namenode_user_admin_default_customer_order_relation_hudi]:

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/db_impl.cc:1546] Created column family [hudi_bootstrap_basefile_hdfs:__namenode_user_admin_default_customer_order_relation_hudi] (ID 3)
22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/column_family.cc:475] --------------- Options for column family [hudi_partitions_hdfs:__namenode_user_admin_default_customer_order_relation_hudi]:

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/db_impl.cc:1546] Created column family [hudi_partitions_hdfs:__namenode_user_admin_default_customer_order_relation_hudi] (ID 4)
22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/column_family.cc:475] --------------- Options for column family [hudi_replaced_fghdfs:__namenode_user_admin_default_customer_order_relation_hudi]:

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/db_impl.cc:1546] Created column family [hudi_replaced_fghdfs:__namenode_user_admin_default_customer_order_relation_hudi] (ID 5)
22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/column_family.cc:475] --------------- Options for column family [hudi_pending_clustering_fghdfs:__namenode_user_admin_default_customer_order_relation_hudi]:

22/02/12 19:07:42 INFO RocksDBDAO: From Rocks DB : [db/db_impl.cc:1546] Created column family [hudi_pending_clustering_fghdfs:__namenode_user_admin_default_customer_order_relation_hudi] (ID 6)
22/02/12 19:07:42 INFO RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
22/02/12 19:07:42 INFO RocksDBDAO: Prefix DELETE (query=part=) on hudi_replaced_fghdfs:__namenode_user_admin_default_customer_order_relation_hudi
22/02/12 19:07:42 INFO RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
22/02/12 19:07:42 INFO AbstractTableFileSystemView: Took 2 ms to read  0 instants, 0 replaced file groups
22/02/12 19:07:42 INFO RocksDbBasedFileSystemView: Initializing pending compaction operations. Count=0
22/02/12 19:07:42 INFO RocksDbBasedFileSystemView: Initializing external data file mapping. Count=0
22/02/12 19:07:42 INFO ClusteringUtils: Found 0 files in pending clustering operations
22/02/12 19:07:42 INFO RocksDbBasedFileSystemView: Resetting file groups in pending clustering to ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb, Total file-groups=0
22/02/12 19:07:42 INFO RocksDBDAO: Prefix DELETE (query=part=) on hudi_pending_clustering_fghdfs:__namenode_user_admin_default_customer_order_relation_hudi
22/02/12 19:07:42 INFO RocksDbBasedFileSystemView: Resetting replacedFileGroups to ROCKSDB based file-system view complete
22/02/12 19:07:42 INFO RocksDbBasedFileSystemView: Created ROCKSDB based file-system view at /tmp/hoodie_timeline_rocksdb
22/02/12 19:07:42 INFO RocksDBDAO: Prefix Search for (query=) on hudi_pending_compaction_hdfs:__namenode_user_admin_default_customer_order_relation_hudi. Total Time Taken (msec)=0. Serialization Time taken(micro)=0, num entries=0
22/02/12 19:07:42 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
22/02/12 19:07:42 INFO CleanPlanner: Nothing to clean here. It is already clean
22/02/12 19:07:43 INFO AbstractHoodieWriteClient: Cleaner started
22/02/12 19:07:43 INFO AbstractHoodieWriteClient: Cleaned failed attempts if any
22/02/12 19:07:43 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:43 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:43 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:43 INFO HoodieTableMetaClient: Loading Active commit timeline for hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:43 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20220212190738514__commit__COMPLETED]}
22/02/12 19:07:43 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
22/02/12 19:07:43 INFO FileSystemViewManager: Creating remote first table view
22/02/12 19:07:43 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20220212190738514__commit__COMPLETED]}
22/02/12 19:07:43 INFO HoodieTimelineArchiveLog: No Instants to archive
22/02/12 19:07:43 INFO AbstractHoodieWriteClient: Committed 20220212190738514
22/02/12 19:07:43 INFO MapPartitionsRDD: Removing RDD 35 from persistence list
22/02/12 19:07:43 INFO BlockManager: Removing RDD 35
22/02/12 19:07:43 INFO MapPartitionsRDD: Removing RDD 50 from persistence list
22/02/12 19:07:43 INFO BlockManager: Removing RDD 50
22/02/12 19:07:43 INFO DeltaSync: Commit 20220212190738514 successful!
22/02/12 19:07:43 INFO DeltaSync: Syncing target hoodie table with hive table(customer_order_relation). Hive metastore URL :jdbc:hive2://hiveserver:10000, basePath :hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:43 INFO DeltaSync: Hive Conf => {dfs.block.access.token.lifetime=600, hive.skewjoin.key=100000, hive.index.compact.binary.search=true, mapreduce.map.log.level=INFO, dfs.namenode.lazypersist.file.scrub.interval.sec=300, file.bytes-per-checksum=512, mapreduce.client.completion.pollinterval=5000, yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false, yarn.log-aggregation-enable=false, yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler, dfs.namenode.edit.log.autoroll.check.interval.ms=300000, mapreduce.job.speculative.retry-after-speculate=15000, ipc.client.fallback-to-simple-auth-allowed=false, dfs.client.failover.connection.retries=0, mapreduce.jobtracker.system.dir=${hadoop.tmp.dir}/mapred/system, hive.metastore.event.db.listener.timetolive=86400s, yarn.scheduler.minimum-allocation-mb=1024, mapreduce.task.profile.map.params=${mapreduce.task.profile.params}, mapreduce.tasktracker.dns.interface=default, mapreduce.map.memory.mb=1024, hive.server2.authentication=NONE, dfs.datanode.failed.volumes.tolerated=0, stream.stderr.reporter.prefix=reporter:, dfs.client.slow.io.warning.threshold.ms=30000, hadoop.security.groups.cache.secs=300, dfs.namenode.top.window.num.buckets=10, yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME, hive.metastore.authorization.storage.checks=false, map.sort.class=org.apache.hadoop.util.QuickSort, dfs.namenode.safemode.threshold-pct=0.999f, mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12, datanucleus.storeManagerType=rdbms, dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000, hive.metastore.aggregate.stats.cache.max.writer.wait=5000ms, fs.s3n.block.size=67108864, yarn.resourcemanager.client.thread-count=50, dfs.client.read.shortcircuit=false, mapreduce.job.end-notification.max.retry.interval=5000, hadoop.security.authentication=simple, dfs.client.mmap.retry.timeout.ms=300000, dfs.datanode.readahead.bytes=4194304, mapreduce.jobhistory.max-age-ms=604800000, yarn.app.mapreduce.client-am.ipc.max-retries=3, hive.script.recordwriter=org.apache.hadoop.hive.ql.exec.TextRecordWriter, hive.mapjoin.followby.map.aggr.hash.percentmemory=0.3, hive.multigroupby.singlereducer=true, yarn.nodemanager.sleep-delay-before-sigkill.ms=250, hive.vectorized.execution.reduce.groupby.enabled=true, fs.trash.interval=0, dfs.datanode.max.locked.memory=0, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, fs.har.impl=org.apache.hadoop.hive.shims.HiveHarFileSystem, dfs.datanode.ipc.address=0.0.0.0:50020, dfs.namenode.delegation.token.renew-interval=86400000, yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088, hive.spark.client.rpc.sasl.mechanisms=DIGEST-MD5, hive.optimize.index.groupby=false, mapreduce.shuffle.max.connections=0, yarn.nodemanager.local-cache.max-files-per-directory=8192, dfs.namenode.replication.work.multiplier.per.iteration=2, mapreduce.jobtracker.address=local, dfs.datanode.slow.io.warning.threshold.ms=300, yarn.client.max-cached-nodemanagers-proxies=0, mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst, hadoop.registry.zk.quorum=localhost:2181, yarn.app.mapreduce.am.job.committer.commit-window=10000, hadoop.registry.zk.session.timeout.ms=60000, hive.tez.exec.print.summary=false, dfs.namenode.heartbeat.recheck-interval=300000, nfs.exports.allowed.hosts=* rw, hive.metastore.filter.hook=org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl, hive.metastore.integral.jdo.pushdown=false, hadoop.util.hash.type=murmur, dfs.namenode.replication.min=1, hive.cli.print.current.db=false, hive.debug.localtask=false, hive.security.authorization.enabled=false, hive.warehouse.subdir.inherit.perms=true, mapreduce.jobtracker.retiredjobs.cache.size=1000, s3.client-write-packet-size=65536, yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030, hive.metastore.try.direct.sql=true, fs.s3.block.size=67108864, hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0})), dfs.permissions.superusergroup=supergroup, yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031, hive.execution.engine=mr, yarn.sharedcache.root-dir=/sharedcache, hive.script.operator.env.blacklist=hive.txn.valid.txns,hive.script.operator.env.blacklist, hive.server2.long.polling.timeout=5000ms, yarn.resourcemanager.zk-timeout-ms=10000, hive.orc.splits.include.file.footer=false, dfs.namenode.http-address=0.0.0.0:50070, yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1, hive.server2.async.exec.wait.queue.size=100, mapreduce.client.output.filter=FAILED, mapreduce.reduce.shuffle.memory.limit.percent=0.25, nfs.rtmax=1048576, hive.metastore.aggregate.stats.cache.max.full=0.9, hive.default.fileformat=TextFile, hadoop.ssl.client.conf=ssl-client.xml, fs.ftp.host=0.0.0.0, hive.optimize.groupby=true, hadoop.http.authentication.simple.anonymous.allowed=true, nfs.server.port=2049, hive.optimize.remove.identity.project=true, dfs.ha.log-roll.period=120, hadoop.security.kms.client.authentication.retry-count=1, mapreduce.task.io.sort.factor=10, dfs.datanode.https.address=0.0.0.0:50475, yarn.sharedcache.uploader.server.address=0.0.0.0:8046, hive.limit.query.max.table.partition=-1, hive.unlock.numretries=10, hive.exec.job.debug.capture.stacktraces=true, hive.hmshandler.retry.interval=2000ms, mapreduce.job.reduces=-1, yarn.nodemanager.recovery.compaction-interval-secs=3600, mapreduce.tasktracker.reduce.tasks.maximum=2, dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}, mapreduce.jobhistory.cleaner.interval-ms=86400000, hive.stats.atomic=false, yarn.nodemanager.vmem-check-enabled=true, yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$, dfs.namenode.replication.considerLoad=true, hive.cli.print.header=false, ha.health-monitor.sleep-after-disconnect.ms=1000, mapreduce.job.counters.max=120, mapreduce.job.running.reduce.limit=0, s3.blocksize=67108864, hive.exec.default.partition.name=__HIVE_DEFAULT_PARTITION__, hive.mapred.partitioner=org.apache.hadoop.hive.ql.io.DefaultHivePartitioner, hive.groupby.mapaggr.checkinterval=100000, mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000, mapreduce.job.classloader=false, yarn.resourcemanager.connect.max-wait.ms=900000, yarn.resourcemanager.ha.automatic-failover.embedded=true, hive.map.aggr.hash.min.reduction=0.5, mapreduce.jobhistory.joblist.cache.size=20000, mapreduce.task.profile.maps=0-2, dfs.client.short.circuit.replica.stale.threshold.ms=1800000, hive.exec.mode.local.auto.input.files.max=4, hive.compactor.worker.timeout=86400s, hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2, hive.variable.substitute=true, hadoop.ssl.server.conf=ssl-server.xml, yarn.timeline-service.state-store-class=org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore, hive.test.mode.prefix=test_, hive.metastore.dml.events=false, yarn.timeline-service.generic-application-history.max-applications=10000, hive.test.mode.samplefreq=32, mapreduce.input.fileinputformat.list-status.num-threads=1, dfs.namenode.edits.dir=${dfs.namenode.name.dir}, s3.bytes-per-checksum=512, fs.permissions.umask-mode=000, yarn.resourcemanager.proxy-user-privileges.enabled=false, hive.vectorized.execution.reduce.enabled=true, hive.skewjoin.mapjoin.map.tasks=10000, yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10, yarn.http.policy=HTTP_ONLY, dfs.namenode.path.based.cache.refresh.interval.ms=30000, yarn.resourcemanager.zk-acl=world:anyone:rwcda, yarn.nodemanager.linux-container-executor.cgroups.mount=false, ipc.client.connect.max.retries.on.timeouts=45, hive.map.groupby.sorted.testmode=false, hadoop.fuse.connection.timeout=300, dfs.block.access.key.update.interval=600, mapreduce.job.token.tracking.ids.enabled=false, hive.exec.orc.default.row.index.stride=10000, ha.health-monitor.rpc-timeout.ms=45000, hive.script.recordreader=org.apache.hadoop.hive.ql.exec.TextRecordReader, yarn.resourcemanager.fail-fast=${yarn.fail-fast}, hadoop.http.cross-origin.allowed-origins=*, hive.metastore.server.max.message.size=104857600, hive.server2.async.exec.keepalive.time=10s, mapreduce.client.submit.file.replication=10, hive.stats.dbconnectionstring=jdbc:derby:;databaseName=TempStatsStore;create=true, hive.zookeeper.connection.max.retries=3, hadoop.ssl.require.client.cert=false, hive.stats.list.num.entries=10, hive.server2.logging.operation.enabled=true, dfs.datanode.cache.revocation.timeout.ms=900000, hive.server2.thrift.login.timeout=20s, dfs.ha.tail-edits.period=60, yarn.client.nodemanager-connect.retry-interval-ms=10000, dfs.namenode.inotify.max.events.per.rpc=1000, hadoop.rpc.protection=authentication, yarn.resourcemanager.fs.state-store.uri=${hadoop.tmp.dir}/yarn/system/rmstore, hive.exec.dynamic.partition=true, hive.cbo.costmodel.hdfs.write=10.0, dfs.datanode.scan.period.hours=504, dfs.datanode.block.id.layout.upgrade.threads=12, hive.tez.exec.inplace.progress=true, dfs.client.read.shortcircuit.skip.checksum=false, hive.exec.temporary.table.storage=default, hive.cli.pretty.output.num.cols=-1, hive.counters.group.name=HIVE, yarn.sharedcache.store.in-memory.initial-delay-mins=10, dfs.client.file-block-storage-locations.timeout.millis=1000, yarn.client.nodemanager-client-async.thread-pool-max-size=500, hive.exec.orc.block.padding.tolerance=0.05, dfs.domain.socket.path=/var/run/hdfs-sockets/dn, mapreduce.tasktracker.dns.nameserver=default, dfs.namenode.stale.datanode.interval=30000, hive.cbo.costmodel.local.fs.write=4.0, hive.join.cache.size=25000, hive.file.max.footer=100, yarn.timeline-service.address=${yarn.timeline-service.hostname}:10200, mapreduce.app-submission.cross-platform=false, hive.archive.enabled=false, dfs.http.policy=HTTP_ONLY, mapreduce.map.output.compress=false, hive.metastore.archive.intermediate.original=_INTERMEDIATE_ORIGINAL, mapreduce.shuffle.max.threads=0, hive.cluster.delegation.token.store.zookeeper.znode=/hivedelegation, mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done, hive.tez.auto.reducer.parallelism=false, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider, hive.spark.client.connect.timeout=1000ms, hive.mapred.mode=nonstrict, dfs.namenode.reject-unresolved-dn-topology-mapping=false, hadoop.ssl.enabled=false, fs.s3a.connection.establish.timeout=5000, mapreduce.jobtracker.expire.trackers.interval=600000, hive.stats.join.factor=1.1, hive.hwi.listen.port=9999, mapreduce.jobhistory.recovery.enable=false, hive.stats.retries.wait=3000ms, datanucleus.cache.level2.type=none, mapreduce.job.reduce.slowstart.completedmaps=0.05, mapreduce.output.fileoutputformat.compress.type=BLOCK, mapreduce.reduce.shuffle.parallelcopies=5, hive.tez.container.size=-1, yarn.nodemanager.delete.thread-count=4, yarn.timeline-service.client.max-retries=30, hadoop.ssl.enabled.protocols=TLSv1, mapreduce.jobtracker.http.address=0.0.0.0:50030, yarn.nodemanager.resource.memory-mb=8192, hadoop.kerberos.kinit.command=kinit, mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec, dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, hive.metastore.aggregate.stats.cache.fpp=0.01, hive.convert.join.bucket.mapjoin.tez=false, yarn.app.mapreduce.am.container.log.backups=0, mapreduce.task.profile=false, ipc.client.rpc-timeout.ms=0, mapreduce.job.running.map.limit=0, hive.io.rcfile.record.interval=2147483647, hadoop.ssl.hostname.verifier=DEFAULT, hive.map.groupby.sorted=false, stream.stderr.reporter.enabled=true, fs.s3a.connection.ssl.enabled=true, yarn.resourcemanager.scheduler.client.thread-count=50, hive.compute.splits.in.am=true, io.seqfile.local.dir=${hadoop.tmp.dir}/io/local, dfs.datanode.directoryscan.threads=1, hive.exec.counters.pull.interval=1000, yarn.timeline-service.client.best-effort=false, yarn.sharedcache.cleaner.resource-sleep-ms=0, yarn.client.failover-retries=0, mapreduce.input.lineinputformat.linespermap=1, hive.new.job.grouping.set.cardinality=30, yarn.nodemanager.container-monitor.interval-ms=3000, mapreduce.job.queuename=default, dfs.datanode.dns.nameserver=default, javax.jdo.option.NonTransactionalRead=true, mapreduce.map.skip.proc.count.autoincr=true, hive.cli.errors.ignore=false, yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3, hive.exec.mode.local.auto.inputbytes.max=134217728, hive.stageid.rearrange=none, fs.automatic.close=true, mapreduce.framework.name=local, dfs.client.failover.sleep.max.millis=15000, yarn.resourcemanager.max-completed-applications=10000, yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging, hive.vectorized.execution.mapjoin.minmax.enabled=false, hive.server2.thrift.http.path=cliservice, yarn.nm.liveness-monitor.expiry-interval-ms=600000, mapreduce.reduce.shuffle.fetch.retry.enabled=${yarn.nodemanager.recovery.enabled}, datanucleus.autoCreateSchema=true, mapreduce.tasktracker.report.address=127.0.0.1:0, hive.exec.orc.memory.pool=0.5, hive.optimize.metadataonly=true, hive.mapred.reduce.tasks.speculative.execution=true, hive.repl.task.factory=org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory, hive.lockmgr.zookeeper.default.partition.name=__HIVE_DEFAULT_ZOOKEEPER_PARTITION__, hive.conf.validation=true, ftp.client-write-packet-size=65536, hive.metastore.kerberos.principal=hive-metastore/_HOST@EXAMPLE.COM, hive.prewarm.enabled=false, mapreduce.task.io.sort.mb=100, hadoop.security.kms.client.encrypted.key.cache.expiry=43200000, hive.fetch.task.conversion=more, hive.metastore.server.min.threads=200, mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000, ftp.blocksize=67108864, hadoop.registry.jaas.context=Client, yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs, hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider, hive.metastore.server.tcp.keepalive=true, hive.vectorized.groupby.checkinterval=100000, fs.df.interval=60000, mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec, io.skip.checksum.errors=false, hadoop.jetty.logs.serve.aliases=true, hive.exec.dynamic.partition.mode=strict, yarn.nodemanager.remote-app-log-dir-suffix=logs, hive.optimize.bucketingsorting=true, hive.merge.mapredfiles=false, dfs.client.context=default, hive.exec.parallel=false, hive.log.explain.output=false, dfs.namenode.edits.noeditlogchannelflush=false, io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization, mapreduce.reduce.skip.maxgroups=0, dfs.ha.fencing.ssh.connect-timeout=30000, hive.tez.cpu.vcores=-1, mapreduce.jobhistory.client.thread-count=10, hadoop.security.random.device.file.path=/dev/urandom, fs.s3a.max.total.tasks=1000, hive.cbo.costmodel.extended=false, hive.binary.record.max.length=1000, hive.multi.insert.move.tasks.share.dependencies=false, hive.optimize.index.filter.compact.minsize=5368709120, yarn.nodemanager.windows-container.memory-limit.enabled=false, mapreduce.job.speculative.speculative-cap-total-tasks=0.01, hive.compactor.abortedtxn.threshold=1000, dfs.datanode.bp-ready.timeout=20, dfs.client-write-packet-size=65536, dfs.journalnode.https-address=0.0.0.0:8481, dfs.namenode.enable.retrycache=true, hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ, yarn.timeline-service.leveldb-state-store.path=${hadoop.tmp.dir}/yarn/timeline, hive.udtf.auto.progress=false, dfs.journalnode.rpc-address=0.0.0.0:8485, hive.exec.orc.encoding.strategy=SPEED, hive.stats.fetch.column.stats=false, yarn.resourcemanager.zk-num-retries=1000, yarn.sharedcache.app-checker.class=org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker, hive.exec.reducers.bytes.per.reducer=256000000, hive.reorder.nway.joins=true, yarn.app.mapreduce.am.container.log.limit.kb=0, mapreduce.tasktracker.map.tasks.maximum=2, hive.io.rcfile.tolerate.corruptions=false, hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding, yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500, yarn.app.mapreduce.am.hard-kill-timeout-ms=10000, dfs.encrypt.data.transfer.cipher.key.bitlength=128, yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC, hive.compat=0.12, file.replication=1, dfs.datanode.drop.cache.behind.writes=false, dfs.client.failover.max.attempts=15, hive.index.compact.query.max.entries=10000000, rpc.metrics.quantile.enable=false, hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled=false, yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0, hive.jobname.length=50, hive.mapjoin.bucket.cache.size=100, dfs.namenode.support.allow.format=true, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, io.file.buffer.size=65536, io.native.lib.available=true, hive.limit.pushdown.memory.usage=-1.0, dfs.client.mmap.cache.size=256, dfs.namenode.delegation.key.update-interval=86400000, hive.vectorized.execution.mapjoin.native.multikey.only.enabled=false, hadoop.user.group.static.mapping.overrides=dr.who=;, yarn.resourcemanager.ha.automatic-failover.enabled=true, hive.script.operator.truncate.env=false, yarn.sharedcache.client-server.thread-count=50, yarn.fail-fast=false, hive.hashtable.key.count.adjustment=1.0, hive.exec.rcfile.use.explicit.header=true, hive.server2.global.init.file.location=${env:HIVE_CONF_DIR}, mapreduce.job.end-notification.retry.attempts=0, hive.exec.stagingdir=.hive-staging, hive.metastore.schema.verification.record.version=true, dfs.client.https.need-auth=false, mapreduce.jobtracker.taskscheduler=org.apache.hadoop.mapred.JobQueueTaskScheduler, yarn.resourcemanager.system-metrics-publisher.enabled=false, hive.server2.async.exec.threads=100, hive.optimize.skewjoin.compiletime=false, dfs.client.https.keystore.resource=ssl-client.xml, dfs.namenode.checkpoint.txns=1000000, yarn.timeline-service.http-authentication.type=simple, hive.metastore.server.max.threads=1000, mapreduce.jobhistory.loadedjobs.cache.size=5, yarn.nodemanager.resource.percentage-physical-cpu-limit=100, yarn.nodemanager.recovery.dir=${hadoop.tmp.dir}/yarn-nm-recovery, dfs.namenode.name.dir=file://${hadoop.tmp.dir}/dfs/name, hive.merge.sparkfiles=false, hive.metastore.try.direct.sql.ddl=true, hive.server2.idle.session.check.operation=true, mapreduce.input.fileinputformat.split.minsize.per.rack=1, mapreduce.cluster.acls.enabled=false, mapreduce.client.progressmonitor.pollinterval=1000, file.client-write-packet-size=65536, hadoop.security.group.mapping.ldap.search.attr.group.name=cn, dfs.namenode.invalidate.work.pct.per.iteration=0.32f, dfs.datanode.dns.interface=default, yarn.nodemanager.resource.cpu-vcores=8, hive.exec.infer.bucket.sort.num.buckets.power.two=false, javax.jdo.option.DetachAllOnCommit=true, yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10, dfs.namenode.secondary.https-address=0.0.0.0:50091, hive.exec.compress.output=false, yarn.timeline-service.client.retry-interval-ms=1000, mapreduce.shuffle.port=13562, yarn.resourcemanager.container.liveness-monitor.interval-ms=600000, hive.exim.uri.scheme.whitelist=hdfs,pfile, hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager, hive.display.partition.cols.separately=true, yarn.scheduler.maximum-allocation-mb=8192, hive.lock.mapred.only.operation=false, mapreduce.job.speculative.minimum-allowed-tasks=10, hive.hbase.generatehfiles=false, hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat, mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController, mapreduce.jobhistory.datestring.cache.size=200000, yarn.resourcemanager.admin.client.thread-count=1, yarn.timeline-service.leveldb-timeline-store.path=${hadoop.tmp.dir}/yarn/timeline, mapreduce.job.reducer.preempt.delay.sec=0, s3native.client-write-packet-size=65536, yarn.nodemanager.localizer.client.thread-count=5, mapreduce.task.userlog.limit.kb=0, mapreduce.jobhistory.minicluster.fixed.ports=false, mapreduce.job.userlog.retain.hours=24, hive.conf.restricted.list=hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role, mapreduce.job.committer.task.cleanup.needed=false, dfs.namenode.block-placement-policy.default.prefer-local-node=true, hive.mapjoin.check.memory.rows=100000, dfs.namenode.fs-limits.max-blocks-per-file=1048576, yarn.client.application-client-protocol.poll-interval-ms=200, hive.exec.orc.default.stripe.size=67108864, hive.decode.partition.name=false, hadoop.registry.secure=false, yarn.sharedcache.store.in-memory.staleness-period-mins=10080, hive.parquet.timestamp.skip.conversion=true, hive.outerjoin.supports.filters=true, hive.groupby.orderby.position.alias=false, yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, hive.server2.logging.operation.level=EXECUTION, dfs.heartbeat.interval=3, mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s, yarn.nodemanager.address=${yarn.nodemanager.hostname}:0, dfs.namenode.fs-limits.max-xattr-size=16384, mapreduce.jobhistory.recovery.store.leveldb.path=${hadoop.tmp.dir}/mapred/history/recoverystore, mapreduce.shuffle.connection-keep-alive.enable=false, hive.stats.deserialization.factor=1.0, yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000, hive.metastore.thrift.framed.transport.enabled=false, dfs.namenode.write.stale.datanode.ratio=0.5f, hive.exec.script.maxerrsize=100000, hive.stats.key.prefix.reserve.length=24, ha.failover-controller.cli-check.rpc-timeout.ms=20000, yarn.sharedcache.cleaner.period-mins=1440, hive.index.compact.file.ignore.hdfs=false, fs.s3a.threads.keepalivetime=60, mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory, hive.optimize.sort.dynamic.partition=false, hadoop.work.around.non.threadsafe.getpwuid=false, mapreduce.jobtracker.taskcache.levels=2, hive.security.metastore.authorization.auth.reads=true, hive.server2.thrift.sasl.qop=auth, hive.zookeeper.session.timeout=1200000ms, hive.server2.thrift.max.message.size=104857600, yarn.resourcemanager.nodemanager-connect-retries=10, mapreduce.jobtracker.staging.root.dir=${hadoop.tmp.dir}/mapred/staging, hive.metastore.stats.ndv.densityfunction=false, yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000, hive.metastore.aggregate.stats.cache.size=10000, mapreduce.tasktracker.healthchecker.interval=60000, dfs.namenode.list.cache.directives.num.responses=100, dfs.blockreport.initialDelay=0, mapreduce.job.speculative.retry-after-no-speculate=1000, hive.in.test=false, hive.exec.rowoffset=false, hive.optimize.index.filter.compact.maxsize=-1, mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate, mapreduce.map.cpu.vcores=1, datanucleus.plugin.pluginRegistryBundleCheck=LOG, yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600, dfs.blockreport.split.threshold=1000000, mapreduce.jobtracker.restart.recover=false, hadoop.registry.zk.connection.timeout.ms=15000, yarn.sharedcache.webapp.address=0.0.0.0:8788, hive.enforce.bucketmapjoin=false, hive.server2.max.start.attempts=30, dfs.namenode.resource.checked.volumes.minimum=1, ftp.replication=3, io.compression.codec.bzip2.library=system-native, dfs.encrypt.data.transfer=false, hive.cbo.costmodel.local.fs.read=4.0, mapreduce.reduce.shuffle.retry-delay.max.ms=60000, hive.spark.client.rpc.threads=8, yarn.app.mapreduce.shuffle.log.limit.kb=0, hive.server2.thrift.max.worker.threads=500, hive.exec.concatenate.check.index=true, hive.vectorized.groupby.maxentries=1000000, mapreduce.task.timeout=600000, hive.optimize.constant.propagation=true, hive.metastore.disallow.incompatible.col.type.changes=false, fs.s3n.multipart.uploads.enabled=false, mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService, hadoop.security.groups.cache.warn.after.ms=5000, yarn.nodemanager.localizer.fetch.thread-count=4, mapreduce.reduce.shuffle.input.buffer.percent=0.70, dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data, dfs.namenode.accesstime.precision=3600000, dfs.namenode.decommission.max.concurrent.tracked.nodes=100, dfs.namenode.avoid.write.stale.datanode=false, yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor, hive.server2.use.SSL=false, mapreduce.jobtracker.heartbeats.in.second=100, hive.exec.script.allow.partial.consumption=false, yarn.nodemanager.windows-container.cpu-limit.enabled=false, dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000, hive.metastore.aggregate.stats.cache.max.partitions=10000, yarn.resourcemanager.fs.state-store.num-retries=0, hive.cbo.costmodel.hdfs.read=1.5, datanucleus.connectionPoolingType=BONECP, hive.server.tcp.keepalive=true, dfs.datanode.use.datanode.hostname=false, hive.security.authorization.sqlstd.confwhitelist=hive\.auto\..*|hive\.cbo\..*|hive\.convert\..*|hive\.exec\.dynamic\.partition.*|hive\.exec\..*\.dynamic\.partitions\..*|hive\.exec\.compress\..*|hive\.exec\.infer\..*|hive\.exec\.mode.local\..*|hive\.exec\.orc\..*|hive\.exec\.parallel.*|hive\.explain\..*|hive\.fetch.task\..*|hive\.groupby\..*|hive\.hbase\..*|hive\.index\..*|hive\.index\..*|hive\.intermediate\..*|hive\.join\..*|hive\.limit\..*|hive\.log\..*|hive\.mapjoin\..*|hive\.merge\..*|hive\.optimize\..*|hive\.orc\..*|hive\.outerjoin\..*|hive\.parquet\..*|hive\.ppd\..*|hive\.prewarm\..*|hive\.server2\.proxy\.user|hive\.skewjoin\..*|hive\.smbjoin\..*|hive\.stats\..*|hive\.tez\..*|hive\.vectorized\..*|mapred\.map\..*|mapred\.reduce\..*|mapred\.output\.compression\.codec|mapred\.job\.queuename|mapred\.output\.compression\.type|mapred\.min\.split\.size|mapreduce\.job\.reduce\.slowstart\.completedmaps|mapreduce\.job\.queuename|mapreduce\.job\.tags|mapreduce\.input\.fileinputformat\.split\.minsize|mapreduce\.map\..*|mapreduce\.reduce\..*|mapreduce\.output\.fileoutputformat\.compress\.codec|mapreduce\.output\.fileoutputformat\.compress\.type|tez\.am\..*|tez\.task\..*|tez\.runtime\..*|tez.queue.name|hive\.exec\.reducers\.bytes\.per\.reducer|hive\.client\.stats\.counters|hive\.exec\.default\.partition\.name|hive\.exec\.drop\.ignorenonexistent|hive\.counters\.group\.name|hive\.default\.fileformat\.managed|hive\.enforce\.bucketing|hive\.enforce\.bucketmapjoin|hive\.enforce\.sorting|hive\.enforce\.sortmergebucketmapjoin|hive\.cache\.expr\.evaluation|hive\.hashtable\.loadfactor|hive\.hashtable\.initialCapacity|hive\.ignore\.mapjoin\.hint|hive\.limit\.row\.max\.size|hive\.mapred\.mode|hive\.map\.aggr|hive\.compute\.query\.using\.stats|hive\.exec\.rowoffset|hive\.variable\.substitute|hive\.variable\.substitute\.depth|hive\.autogen\.columnalias\.prefix\.includefuncname|hive\.autogen\.columnalias\.prefix\.label|hive\.exec\.check\.crossproducts|hive\.compat|hive\.exec\.concatenate\.check\.index|hive\.display\.partition\.cols\.separately|hive\.error\.on\.empty\.partition|hive\.execution\.engine|hive\.exim\.uri\.scheme\.whitelist|hive\.file\.max\.footer|hive\.mapred\.supports\.subdirectories|hive\.insert\.into\.multilevel\.dirs|hive\.localize\.resource\.num\.wait\.attempts|hive\.multi\.insert\.move\.tasks\.share\.dependencies|hive\.support\.quoted\.identifiers|hive\.resultset\.use\.unique\.column\.names|hive\.analyze\.stmt\.collect\.partlevel\.stats|hive\.server2\.logging\.operation\.level|hive\.support\.sql11\.reserved\.keywords|hive\.exec\.job\.debug\.capture\.stacktraces|hive\.exec\.job\.debug\.timeout|hive\.exec\.max\.created\.files|hive\.exec\.reducers\.max|hive\.reorder\.nway\.joins|hive\.output\.file\.extension|hive\.exec\.show\.job\.failure\.debug\.info|hive\.exec\.tasklog\.debug\.timeout, hive.server2.tez.sessions.per.default.queue=1, file.blocksize=67108864, ipc.client.kill.max=10, hive.metastore.schema.verification=false, yarn.resourcemanager.nodemanager.minimum.version=NONE, dfs.namenode.list.cache.pools.num.responses=100, dfs.datanode.cache.revocation.polling.ms=500, hive.metastore.batch.retrieve.max=300, hive.querylog.location=/var/folders/p_/5jfy_yk16kb_m48kn65vrqd40000gn/T//mozhenghua, dfs.client.read.shortcircuit.streams.cache.size=256, io.mapfile.bloom.size=1048576, hive.support.quoted.identifiers=column, hive.exec.orc.dictionary.key.size.threshold=0.8, net.topology.impl=org.apache.hadoop.net.NetworkTopology, dfs.namenode.kerberos.principal.pattern=*, fs.trash.checkpoint.interval=0, s3.replication=3, hive.server2.thrift.http.cookie.max.age=86400s, yarn.app.mapreduce.client.max-retries=3, mapreduce.job.maps=2, dfs.namenode.http-address.namenode.namenode295=namenode:50070, yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000, hive.server2.zookeeper.namespace=hiveserver2, yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler, datanucleus.validateTables=false, mapreduce.reduce.maxattempts=4, mapreduce.job.acl-view-job= , hive.server2.thrift.http.cookie.is.secure=true, dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary, hive.lazysimple.extended_boolean_literal=false, hadoop.http.authentication.type=simple, hadoop.security.java.secure.random.algorithm=SHA1PRNG, hive.stats.autogather=true, dfs.namenode.resource.du.reserved=104857600, yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25, yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, hive.metastore.warehouse.dir=file:/Users/mozhenghua/j2ee_solution/project/plugins/tis-datax/tis-datax-hudi-plugin/spark-warehouse, mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer, hive.exec.orc.skip.corrupt.data=false, yarn.timeline-service.ttl-ms=604800000, dfs.blocksize=134217728, dfs.nameservices=namenode, yarn.sharedcache.admin.thread-count=1, tfile.io.chunk.size=1048576, hive.entity.separator=@, mapreduce.task.combine.progress.records=10000, hive.exec.local.scratchdir=/var/folders/p_/5jfy_yk16kb_m48kn65vrqd40000gn/T//mozhenghua, yarn.sharedcache.cleaner.initial-delay-mins=10, fs.ftp.host.port=21, mapreduce.job.committer.setup.cleanup.needed=false, yarn.timeline-service.hostname=0.0.0.0, hive.limit.row.max.size=100000, javax.jdo.option.Multithreaded=true, hive.mapjoin.hybridgrace.minnumpartitions=16, hive.server2.session.check.interval=6h, hive.downloaded.resources.dir=/var/folders/p_/5jfy_yk16kb_m48kn65vrqd40000gn/T//${hive.session.id}_resources, mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab, yarn.resourcemanager.leveldb-state-store.path=${hadoop.tmp.dir}/yarn/system/rmstore, hive.zookeeper.connection.basesleeptime=1000ms, ha.health-monitor.connect-retry-interval.ms=1000, dfs.namenode.max.objects=0, hive.exec.infer.bucket.sort=false, hive.ignore.mapjoin.hint=true, yarn.resourcemanager.amlauncher.thread-count=50, hive.metastore.archive.intermediate.archived=_INTERMEDIATE_ARCHIVED, nfs.wtmax=1048576, hive.zookeeper.client.port=2181, dfs.client.block.write.retries=3, yarn.acl.enable=false, hive.vectorized.execution.mapjoin.overflow.repeated.threshold=-1, hive.metastore.orm.retrieveMapNullsAsEmptyStrings=false, fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs, hive.metastore.aggregate.stats.cache.max.variance=0.01, hive.compactor.worker.threads=0, hive.analyze.stmt.collect.partlevel.stats=true, dfs.block.scanner.volume.bytes.per.second=1048576, dfs.namenode.fs-limits.max-component-length=255, hive.map.aggr.hash.percentmemory=0.5, hive.explain.user=false, mapreduce.job.complete.cancel.delegation.tokens=true, hive.io.rcfile.record.buffer.size=4194304, yarn.resourcemanager.recovery.enabled=false, yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000, hive.metastore.aggregate.stats.cache.max.reader.wait=1000ms, mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, hive.server2.enable.doAs=true, fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs, hive.hmshandler.retry.attempts=10, yarn.nodemanager.process-kill-wait.ms=2000, ha.zookeeper.parent-znode=/hadoop-ha, hive.smbjoin.cache.rows=10000, hive.hbase.wal.enabled=true, yarn.admin.acl=*, s3native.blocksize=67108864, hive.server2.logging.operation.log.location=/var/folders/p_/5jfy_yk16kb_m48kn65vrqd40000gn/T//mozhenghua/operation_logs, fs.s3a.threads.max=256, yarn.resourcemanager.state-store.max-completed-applications=${yarn.resourcemanager.max-completed-applications}, fs.AbstractFileSystem.ftp.impl=org.apache.hadoop.fs.ftp.FtpFs, ipc.client.idlethreshold=4000, hive.metastore.thrift.compact.protocol.enabled=false, ftp.bytes-per-checksum=512, dfs.namenode.http-address.namenode.namenode228=namenode:50070, hive.mapjoin.hybridgrace.memcheckfrequency=1024, hive.start.cleanup.scratchdir=false, hive.metastore.aggregate.stats.cache.clean.until=0.8, hive.server2.support.dynamic.service.discovery=false, hive.vectorized.groupby.flush.percent=0.1, hive.exec.drop.ignorenonexistent=true, ipc.client.connection.maxidletime=10000, hive.merge.smallfiles.avgsize=16000000, hive.metastore.connect.retries=3, hadoop.common.configuration.version=0.23.0, hive.orc.row.index.stride.dictionary.check=true, mapreduce.ifile.readahead.bytes=4194304, io.map.index.interval=128, yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000, javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=metastore_db;create=true, hive.exec.show.job.failure.debug.info=true, tfile.fs.output.buffer.size=262144, hive.vectorized.execution.mapjoin.native.enabled=true, hadoop.security.group.mapping.ldap.search.attr.member=member, yarn.nodemanager.health-checker.script.timeout-ms=1200000, hive.server2.tez.initialize.default.sessions=false, dfs.namenode.handler.count=10, hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator, hive.tez.max.partition.factor=2.0, hive.join.emit.interval=1000, hive.server2.thrift.port=10000, dfs.namenode.fs-limits.min-block-size=1048576, dfs.datanode.handler.count=10, hive.mapjoin.followby.gby.localtask.max.memory.usage=0.55, hive.sample.seednumber=0, hive.default.fileformat.managed=none, yarn.resourcemanager.connect.retry-interval.ms=30000, hive.optimize.ppd.storage=true, yarn.resourcemanager.hostname=0.0.0.0, nfs.dump.dir=/tmp/.hdfs-nfs, hive.spark.client.future.timeout=60s, hive.script.auto.progress=false, hive.exec.parallel.thread.number=8, dfs.namenode.retrycache.heap.percent=0.03f, dfs.namenode.xattrs.enabled=true, mapreduce.jobhistory.webapp.address=0.0.0.0:19888, hive.optimize.index.autoupdate=false, mapreduce.job.speculative.slowtaskthreshold=1.0, yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032, hive.map.aggr.hash.force.flush.memory.threshold=0.9, hive.exec.submitviachild=false, datanucleus.fixedDatastore=false, fs.s3a.connection.timeout=50000, hive.in.tez.test=false, hive.orc.compute.splits.num.threads=10, dfs.block.access.token.enable=false, yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400, hive.metastore.sasl.enabled=false, mapreduce.map.sort.spill.percent=0.80, mapreduce.job.end-notification.max.attempts=5, dfs.namenode.safemode.extension=30000, hive.script.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, hive.heartbeat.interval=1000, hive.mapred.local.mem=0, yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore, hadoop.job.ugi=admin, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, mapreduce.task.profile.reduces=0-2, dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager, hive.tez.dynamic.partition.pruning.max.data.size=104857600, ha.failover-controller.graceful-fence.connection.retries=1, mapreduce.job.jvm.numtasks=1, hive.optimize.index.filter=false, hive.exec.orc.default.compress=ZLIB, hive.mapjoin.localtask.max.memory.usage=0.9, hive.exec.orc.default.block.size=268435456, dfs.namenode.decommission.blocks.per.interval=500000, dfs.image.compress=false, fs.s3n.multipart.copy.block.size=5368709120, yarn.am.liveness-monitor.expiry-interval-ms=600000, mapreduce.job.ubertask.maxreduces=1, dfs.namenode.safemode.min.datanodes=0, fs.s3a.attempts.maximum=10, parquet.memory.pool.ratio=0.5, mapreduce.jobhistory.move.interval-ms=180000, mapreduce.tasktracker.outofband.heartbeat=false, hive.metastore.uris=thrift://hiveserver:9083, fs.s3.maxRetries=4, fs.s3a.multipart.size=104857600, hive.compactor.initiator.on=false, hive.mapjoin.smalltable.filesize=25000000, hive.metastore.client.connect.retry.delay=1s, yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0, mapreduce.shuffle.ssl.enabled=false, dfs.namenode.backup.address=0.0.0.0:50100, yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody, dfs.namenode.checkpoint.check.period=60, nfs.allow.insecure.ports=true, hive.cache.expr.evaluation=true, hive.ssl.protocol.blacklist=SSLv2,SSLv3, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping, hive.exec.max.created.files=100000, fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem, yarn.app.mapreduce.shuffle.log.backups=0, hive.exec.scratchdir=/tmp/hive, datanucleus.rdbms.useLegacyNativeValueStrategy=true, dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0, yarn.nodemanager.container-metrics.unregister-delay-ms=10000, dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$, fs.defaultFS=hdfs://namenode, mapreduce.reduce.merge.inmem.threshold=1000, hive.enforce.sorting=false, dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f, hive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator, io.map.index.skip=0, dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, hive.rpc.query.plan=false, yarn.app.mapreduce.am.resource.mb=1536, hive.serdes.using.metastore.for.schema=org.apache.hadoop.hive.ql.io.orc.OrcSerde,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe,org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe,org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe,org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe, hive.session.history.enabled=false, hive.ppd.recognizetransivity=true, hive.exec.tasklog.debug.timeout=20000, yarn.nodemanager.disk-health-checker.interval-ms=120000, mapreduce.reduce.memory.mb=1024, mapreduce.job.maxtaskfailures.per.tracker=3, hadoop.security.crypto.buffer.size=8192, yarn.scheduler.minimum-allocation-vcores=1, yarn.resourcemanager.keytab=/etc/krb5.keytab, yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn, yarn.resourcemanager.fs.state-store.retry-interval-ms=1000, hive.server2.thrift.http.worker.keepalive.time=60s, hive.server2.table.type.mapping=CLASSIC, hive.exec.job.debug.timeout=30000, hive.exec.rcfile.use.sync.cache=true, hive.optimize.correlation=false, mapreduce.job.split.metainfo.maxsize=10000000, hive.exec.mode.local.auto=false, hive.skewjoin.mapjoin.min.split=33554432, dfs.client.use.legacy.blockreader=false, hive.tez.min.partition.factor=0.25, mapreduce.reduce.shuffle.read.timeout=180000, yarn.app.mapreduce.task.container.log.backups=0, hive.stats.gather.num.threads=10, hive.entity.capture.transform=false, hive.exec.orc.default.block.padding=true, ipc.client.connect.max.retries=10, fs.s3.buffer.dir=${hadoop.tmp.dir}/s3, yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore, hive.index.compact.query.max.size=10737418240, hive.auto.convert.join.noconditionaltask=true, hive.exec.perf.logger=org.apache.hadoop.hive.ql.log.PerfLogger, hive.server2.thrift.min.worker.threads=5, dfs.datanode.max.transfer.threads=4096, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, mapreduce.input.fileinputformat.input.dir.recursive=false, hive.log.every.n.records=0, hive.tez.dynamic.partition.pruning=true, hadoop.http.authentication.token.validity=36000, hive.optimize.bucketmapjoin=false, dfs.namenode.secondary.http-address=0.0.0.0:50090, mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst, hadoop.fuse.timer.period=5, yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider, yarn.nodemanager.resourcemanager.minimum.version=NONE, mapreduce.map.skip.maxrecords=0, hive.compactor.delta.num.threshold=10, dfs.namenode.top.num.users=10, yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider, mapreduce.tasktracker.indexcache.mb=10, hive.localize.resource.num.wait.attempts=5, hadoop.tmp.dir=/tmp/hadoop-${user.name}, dfs.webhdfs.enabled=true, datanucleus.autoStartMechanismMode=checked, yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090, hive.enforce.sortmergebucketmapjoin=false, hive.limit.optimize.limit.file=10, dfs.namenode.avoid.read.stale.datanode=false, mapreduce.cluster.local.dir=${hadoop.tmp.dir}/mapred/local, mapreduce.jobhistory.move.thread-count=3, yarn.sharedcache.store.in-memory.check-period-mins=720, fs.s3a.multipart.threshold=2147483647, hive.cli.prompt=hive, hive.metastore.cache.pinobjtypes=Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order, hive.hmshandler.force.reload.conf=false, hive.metastore.fs.handler.class=org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl, dfs.namenode.checkpoint.period=3600, hive.spark.job.monitor.timeout=60s, hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f, dfs.replication=2, dfs.namenode.datanode.registration.ip-hostname-check=true, mapreduce.job.reducer.unconditional-preempt.delay.sec=300, dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp, yarn.timeline-service.recovery.enabled=false, hive.metastore.execute.setugi=true, hadoop.security.instrumentation.requires.admin=false, dfs.namenode.startup.delay.block.deletion.sec=0, yarn.resourcemanager.resource-tracker.client.thread-count=50, hive.stats.reliable=false, ha.failover-controller.new-active.rpc-timeout.ms=60000, hive.support.concurrency=false, ipc.server.max.connections=0, yarn.app.mapreduce.am.job.task.listener.thread-count=30, yarn.scheduler.maximum-allocation-vcores=32, net.topology.script.number.args=100, mapreduce.tasktracker.healthchecker.script.timeout=600000, mapreduce.reduce.cpu.vcores=1, ftp.stream-buffer-size=4096, yarn.client.nodemanager-connect.max-wait-ms=180000, hive.int.timestamp.conversion.in.seconds=false, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, ha.zookeeper.acl=world:anyone:rwcda, hive.server2.idle.session.timeout=7d, ipc.client.connect.retry.interval=1000, hive.mapjoin.optimized.hashtable.wbsize=10485760, mapreduce.reduce.speculative=true, hive.transform.escape.input=false, dfs.namenode.name.dir.restore=false, mapreduce.reduce.shuffle.merge.percent=0.66, yarn.sharedcache.nested-level=3, datanucleus.validateColumns=false, yarn.nodemanager.webapp.cross-origin.enabled=false, mapreduce.job.ubertask.enable=false, hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret, dfs.namenode.top.enabled=true, hive.exec.orc.default.buffer.size=262144, dfs.bytes-per-checksum=512, hive.compactor.check.interval=300s, dfs.namenode.fs-limits.max-xattrs-per-inode=32, dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec, yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false, mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD, hive.metastore.direct.sql.batch.size=0, mapreduce.jobtracker.persist.jobstatus.active=true, hive.compute.query.using.stats=false, hive.querylog.enable.plan.progress=true, hive.merge.mapfiles=true, hive.stats.fetch.partition.stats=true, dfs.client.use.datanode.hostname=true, hive.security.command.whitelist=set,reset,dfs,add,list,delete,reload,compile, yarn.timeline-service.webapp.https.address=${yarn.timeline-service.hostname}:8190, tfile.fs.input.buffer.size=262144, dfs.storage.policy.enabled=true, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX, hive.exec.orc.split.strategy=HYBRID, yarn.resourcemanager.am.max-attempts=2, hive.txn.timeout=300s, mapreduce.job.emit-timeline-data=false, mapreduce.jobhistory.cleaner.enable=true, hive.scratch.dir.permission=700, ha.failover-controller.graceful-fence.rpc-timeout.ms=5000, dfs.datanode.balance.bandwidthPerSec=1048576, io.mapfile.bloom.error.rate=0.005, dfs.client.block.write.replace-datanode-on-failure.enable=true, dfs.client.mmap.cache.timeout.ms=3600000, hive.merge.rcfile.block.level=true, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, dfs.namenode.blocks.per.postponedblocks.rescan=10000, yarn.sharedcache.client-server.address=0.0.0.0:8045, yarn.app.mapreduce.am.command-opts=-Xmx1024m, hive.hadoop.supports.splittable.combineinputformat=false, hive.zookeeper.clean.extra.nodes=false, hive.stats.collect.tablekeys=false, mapreduce.reduce.skip.proc.count.autoincr=true, dfs.namenode.checkpoint.max-retries=3, javax.jdo.option.ConnectionUserName=APP, dfs.namenode.path.based.cache.retry.interval.ms=30000, hive.query.result.fileformat=TextFile, mapreduce.jobtracker.maxtasks.perjob=-1, hive.stats.collect.scancols=false, hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:hdfs@, dfs.client.failover.proxy.provider.namenode=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider, s3native.stream-buffer-size=4096, mapreduce.task.profile.reduce.params=${mapreduce.task.profile.params}, mapreduce.input.fileinputformat.split.minsize.per.node=1, dfs.permissions.enabled=true, hive.server2.allow.user.substitution=true, ha.health-monitor.check-interval.ms=1000, yarn.sharedcache.enabled=false, hive.compactor.delta.pct.threshold=0.1, hive.server2.thrift.http.cookie.auth.enabled=true, hive.exec.max.dynamic.partitions.pernode=100, yarn.nodemanager.recovery.enabled=false, dfs.namenode.audit.loggers=default, yarn.nodemanager.health-checker.interval-ms=600000, dfs.namenode.acls.enabled=false, hive.stats.ndv.error=20.0, mapreduce.job.acl-modify-job= , yarn.resourcemanager.work-preserving-recovery.enabled=true, yarn.timeline-service.handler-thread-count=10, hive.optimize.skewjoin=false, hadoop.http.cross-origin.enabled=false, hive.merge.size.per.task=256000000, yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000, hive.auto.convert.join.use.nonstaged=false, hive.localize.resource.wait.interval=5000ms, hive.optimize.distinct.rewrite=true, mapreduce.shuffle.transfer.buffer.size=131072, dfs.ha.automatic-failover.enabled.namenode=true, hive.plan.serialization.format=kryo, dfs.namenode.list.encryption.zones.num.responses=100, yarn.app.mapreduce.am.job.committer.cancel-timeout=60000, yarn.nodemanager.hostname=0.0.0.0, hive.rework.mapredwork=false, hive.optimize.sampling.orderby=false, yarn.log-aggregation.retain-check-interval-seconds=-1, hive.merge.orcfile.stripe.level=true, hive.exec.orc.compression.strategy=SPEED, fs.har.impl.disable.cache=true, hive.stats.max.variable.length=100, yarn.timeline-service.http-authentication.simple.anonymous.allowed=true, hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group), hive.mapred.supports.subdirectories=false, mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo, hive.exec.script.trust=false, yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true, yarn.resourcemanager.leveldb-state-store.compaction-interval-secs=3600, yarn.app.mapreduce.am.resource.cpu-vcores=1, hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.MemoryTokenStore, yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000, hive.vectorized.execution.enabled=false, yarn.nodemanager.vmem-pmem-ratio=2.1, yarn.nodemanager.localizer.cache.target-size-mb=10240, hive.stats.key.prefix.max.length=150, hive.cbo.enable=true, hive.optimize.reducededuplication=true, hive.exec.orc.zerocopy=false, dfs.namenode.rpc-address.namenode.namenode295=namenode:8020, yarn.resourcemanager.scheduler.monitor.enable=false, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe, hive.auto.convert.join=true, dfs.client.domain.socket.data.traffic=false, hive.cbo.costmodel.network=150.0, fs.s3a.threads.core=15, mapreduce.tasktracker.http.threads=40, hive.exim.strict.repl.tables=true, yarn.resourcemanager.zk-state-store.parent-path=/rmstore, dfs.datanode.block-pinning.enabled=false, dfs.ha.namenodes.namenode=namenode228,namenode295, hive.metastore.failure.retries=1, mapreduce.map.speculative=true, hive.metastore.aggregate.stats.cache.ttl=600s, yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true, mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000, mapred.output.compression.type=BLOCK, dfs.namenode.https-address.namenode.namenode295=namenode:50470, hive.hwi.listen.host=0.0.0.0, dfs.cachereport.intervalMsec=10000, ipc.maximum.data.length=67108864, hive.orc.cache.stripe.details.size=10000, hive.metastore.batch.retrieve.table.partition.max=1000, yarn.log-aggregation.retain-seconds=-1, hive.server2.thrift.http.cookie.is.httponly=true, hive.metastore.client.socket.timeout=600s, yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042, fs.default.name=hdfs://namenode, yarn.sharedcache.store.class=org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore, dfs.namenode.replication.interval=3, yarn.nodemanager.local-dirs=${hadoop.tmp.dir}/nm-local-dir, nfs.mountd.port=4242, hive.resultset.use.unique.column.names=true, hadoop.registry.zk.retry.interval.ms=1000, hadoop.security.uid.cache.secs=14400, hive.cbo.costmodel.cpu=0.000001, hive.variable.substitute.depth=40, dfs.namenode.resource.check.interval=5000, dfs.datanode.fsdatasetcache.max.threads.per.volume=4, javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory, hive.fetch.output.serde=org.apache.hadoop.hive.serde2.DelimitedJSONSerDe, hive.hashtable.loadfactor=0.75, hive.auto.convert.join.noconditionaltask.size=10000000, fs.s3n.multipart.uploads.block.size=67108864, hive.server2.thrift.exponential.backoff.slot.length=100ms, hive.exec.compress.intermediate=false, hive.support.sql11.reserved.keywords=true, hive.exec.check.crossproducts=true, hive.optimize.listbucketing=false, mapreduce.reduce.shuffle.connect.timeout=180000, mapreduce.am.max-attempts=2, dfs.datanode.http.address=0.0.0.0:50075, hadoop.security.authorization=false, hive.stats.map.num.entries=10, mapreduce.task.merge.progress.records=10000, hive.optimize.sampling.orderby.percent=0.1, hive.metastore.event.clean.freq=0s, dfs.image.transfer.timeout=60000, mapreduce.ifile.readahead=true, mapreduce.task.skip.start.attempts=2, yarn.sharedcache.uploader.server.thread-count=50, yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000, hive.io.rcfile.column.number.conf=0, yarn.resourcemanager.ha.enabled=false, hive.limit.optimize.fetch.max=50000, hive.exec.copyfile.maxsize=33554432, dfs.namenode.rpc-address.namenode.namenode228=namenode:8020, dfs.namenode.servicerpc-address.namenode.namenode295=namenode:8022, mapreduce.input.fileinputformat.split.minsize=1, hive.txn.max.open.batch=1000, yarn.timeline-service.ttl-enable=true, dfs.namenode.top.windows.minutes=1,5,25, hadoop.bin.path=/usr/bin/hadoop, dfs.namenode.backup.http-address=0.0.0.0:50105, hive.auto.convert.sortmerge.join=false, dfs.namenode.delegation.token.max-lifetime=604800000, mapreduce.map.maxattempts=4, datanucleus.cache.level2=false, fs.hdfs.impl=org.apache.hadoop.hdfs.DistributedFileSystem, hive.groupby.skewindata=false, hive.compactor.cleaner.run.interval=5000ms, dfs.image.transfer.bandwidthPerSec=0, dfs.namenode.max.extra.edits.segments.retained=10000, yarn.sharedcache.nm.uploader.replication.factor=10, dfs.client.mmap.enabled=true, hive.fileformat.check=true, mapreduce.job.ubertask.maxmaps=9, fs.client.resolve.remote.symlinks=true, dfs.stream-buffer-size=4096, dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT, yarn.app.mapreduce.shuffle.log.separate=true, hive.tez.smb.number.waves=0.5, hive.server2.transport.mode=binary, io.seqfile.lazydecompress=true, dfs.namenode.https-address.namenode.namenode228=namenode:50470, hadoop.security.group.mapping.ldap.ssl=false, hive.hwi.war.file=${env:HWI_WAR_FILE}, mapreduce.task.files.preserve.failedtasks=false, fs.s3a.paging.maximum=5000, s3native.bytes-per-checksum=512, mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle, hive.spark.client.secret.bits=256, yarn.timeline-service.enabled=false, dfs.journalnode.http-address=0.0.0.0:8480, hive.hbase.snapshot.restoredir=/tmp, hive.server2.thrift.http.port=10001, yarn.nodemanager.keytab=/etc/krb5.keytab, dfs.user.home.dir.prefix=/user, hadoop.http.staticuser.user=dr.who, dfs.ha.automatic-failover.enabled=false, datanucleus.identifierFactory=datanucleus1, mapreduce.jobhistory.http.policy=HTTP_ONLY, mapreduce.jobtracker.persist.jobstatus.hours=1, dfs.blockreport.intervalMsec=21600000, hive.hashtable.initialCapacity=100000, io.seqfile.compress.blocksize=1000000, hive.mapjoin.hybridgrace.hashtable=true, yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033, dfs.client.failover.connection.retries.on.timeouts=0, hadoop.registry.rm.enabled=false, ha.zookeeper.session-timeout.ms=5000, yarn.sharedcache.checksum.algo.impl=org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl, dfs.replication.max=512, yarn.nodemanager.container-manager.thread-count=20, hadoop.security.groups.negative-cache.secs=30, fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem, hive.stats.jdbcdriver=org.apache.derby.jdbc.EmbeddedDriver, hadoop.registry.zk.retry.times=5, hive.merge.tezfiles=false, hive.metastore.event.expiry.duration=0s, file.stream-buffer-size=4096, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, mapreduce.jobhistory.recovery.store.fs.uri=${hadoop.tmp.dir}/mapred/history/recoverystore, dfs.default.chunk.view.size=32768, yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy, hive.cbo.returnpath.hiveop=false, yarn.timeline-service.keytab=/etc/krb5.keytab, hive.optimize.ppd=true, hive.zookeeper.namespace=hive_zookeeper_namespace, mapreduce.reduce.input.buffer.percent=0.0, mapreduce.jobhistory.address=0.0.0.0:10020, io.seqfile.sorter.recordlimit=1000000, dfs.namenode.num.checkpoints.retained=2, mapreduce.job.max.split.locations=10, mapreduce.tasktracker.local.dir.minspacestart=0, mapreduce.reduce.log.level=INFO, yarn.timeline-service.webapp.address=${yarn.timeline-service.hostname}:8188, hive.autogen.columnalias.prefix.includefuncname=false, hive.stats.jdbc.timeout=30s, hive.mapjoin.hybridgrace.minwbsize=524288, hive.optimize.null.scan=true, yarn.resourcemanager.zk-retry-interval-ms=1000, mapred.child.java.opts=-Xmx200m, dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000, hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec, hive.default.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, hive.querylog.plan.progress.interval=60000ms, hive.auto.progress.timeout=0s, mapreduce.job.end-notification.retry.interval=1000, yarn.nodemanager.remote-app-log-dir=/tmp/logs, hadoop.http.cross-origin.allowed-headers=X-Requested-With,Content-Type,Accept,Origin, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.server.read.socket.timeout=10s, dfs.namenode.servicerpc-address.namenode.namenode228=namenode:8022, mapreduce.jobtracker.tasktracker.maxblacklists=4, hadoop.security.group.mapping.ldap.directory.search.timeout=10000, hadoop.http.cross-origin.allowed-methods=GET,POST,HEAD, dfs.namenode.decommission.interval=30, hive.typecheck.on.insert=true, yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400, hive.insert.into.external.tables=true, yarn.nodemanager.delete.debug-delay-sec=0, dfs.namenode.https-address=0.0.0.0:50470, hive.stats.collect.rawdatasize=true, mapreduce.tasktracker.local.dir.minspacekill=0, hive.enforce.bucketing=false, hadoop.http.cross-origin.max-age=1800, dfs.https.server.keystore.resource=ssl-server.xml, yarn.nodemanager.log-aggregation.compression-type=none, mapred.output.compress=true, mapreduce.tasktracker.http.address=0.0.0.0:50060, hive.auto.convert.sortmerge.join.to.mapjoin=false, ipc.client.connect.timeout=20000, hive.insert.into.multilevel.dirs=false, datanucleus.transactionIsolation=read-committed, hive.metadata.move.exported.metadata.to.trash=true, dfs.namenode.path.based.cache.block.map.allocation.percent=0.25, yarn.resourcemanager.webapp.cross-origin.enabled=false, mapreduce.cluster.temp.dir=${hadoop.tmp.dir}/mapred/temp, mapreduce.output.fileoutputformat.compress=true, yarn.client.failover-retries-on-socket-timeouts=0, hive.metastore.client.socket.lifetime=0s, fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a, dfs.client.block.write.replace-datanode-on-failure.best-effort=false, hive.fetch.task.aggr=false, hive.optimize.reducededuplication.min.reducer=4, hive.tez.dynamic.partition.pruning.max.event.size=1048576, hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab, ipc.ping.interval=60000, yarn.sharedcache.nm.uploader.thread-count=20, hive.server2.thrift.http.max.idle.time=1800s, hive.test.mode=false, mapreduce.jobhistory.admin.address=0.0.0.0:10033, yarn.nodemanager.pmem-check-enabled=true, hive.user.install.directory=hdfs:///user/, hive.exec.max.dynamic.partitions=1000, fs.s3a.fast.upload=false, yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker, dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240, hive.ppd.remove.duplicatefilters=true, yarn.sharedcache.admin.address=0.0.0.0:8047, mapreduce.jobtracker.handler.count=10, dfs.client.cached.conn.retry=3, hive.optimize.union.remove=false, fs.s3a.fast.buffer.size=1048576, mapreduce.jobtracker.jobhistory.block.size=3145728, hive.test.authz.sstd.hs2.mode=false, javax.jdo.option.ConnectionPassword=mine, fs.s3.sleepTimeSeconds=10, dfs.datanode.du.reserved=0, hadoop.registry.zk.retry.ceiling.ms=60000, dfs.datanode.address=0.0.0.0:50010, hive.lock.numretries=100, hive.spark.client.server.connect.timeout=90000ms, dfs.namenode.num.extra.edits.retained=1000000, mapreduce.jobhistory.admin.acl=*, hive.security.authorization.task.factory=org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl, dfs.datanode.drop.cache.behind.reads=false, fs.hdfs.impl.disable.cache=false, ipc.client.ping=true, hive.error.on.empty.partition=false, hive.mapper.cannot.span.multiple.partitions=false, datanucleus.validateConstraints=false, dfs.datanode.directoryscan.interval=21600, hive.exec.reducers.max=1009, mapreduce.input.fileinputformat.split.maxsize=256000000, hive.metastore.archive.intermediate.extracted=_INTERMEDIATE_EXTRACTED, hive.script.operator.id.env.var=HIVE_SCRIPT_OPERATOR_ID, yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election, s3native.replication=3, dfs.namenode.retrycache.expirytime.millis=600000, hive.spark.client.rpc.max.size=52428800, fs.s3a.connection.maximum=15, hive.limit.optimize.enable=false, hive.session.silent=false, yarn.app.mapreduce.client.job.max-retries=0, hive.metastore.expression.proxy=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore, hive.tez.log.level=INFO, hive.stats.dbclass=fs, hive.server2.thrift.worker.keepalive.time=60s, mapreduce.job.hdfs-servers=${fs.defaultFS}, mapreduce.job.speculative.speculative-cap-running-tasks=0.1, fs.s3a.multipart.purge.age=86400, mapreduce.jobtracker.jobhistory.lru.cache.size=5, dfs.client.use.legacy.blockreader.local=false, dfs.datanode.sync.behind.writes=false, dfs.client.failover.sleep.base.millis=500, mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000, dfs.datanode.data.dir.perm=700, hive.autogen.columnalias.prefix.label=_c, hive.mapjoin.optimized.hashtable=true, hive.lock.sleep.between.retries=60s, mapreduce.fileoutputcommitter.algorithm.version=1, hive.exec.submit.local.task.via.child=true, hive.metastore.client.drop.partitions.using.expressions=true, dfs.client.datanode-restart.timeout=30, hive.metastore.aggregate.stats.cache.enabled=true, hadoop.hdfs.configuration.version=1, hive.stats.retries.max=0, ipc.server.listen.queue.size=128, mapreduce.shuffle.ssl.file.buffer.size=65536, fs.s3a.multipart.purge=false, hive.prewarm.numcontainers=10, dfs.datanode.hdfs-blocks-metadata.enabled=true, hive.explain.dependency.append.tasktype=false, hive.server2.async.exec.shutdown.timeout=10s, yarn.dispatcher.drain-events.timeout=300000, hive.server2.idle.operation.timeout=5d, hive.optimize.sampling.orderby.number=1000, dfs.namenode.fs-limits.max-directory-items=1048576, yarn.nodemanager.log.retain-seconds=10800, dfs.image.transfer.chunksize=65536, hive.optimize.bucketmapjoin.sortedmerge=false, hive.map.aggr=true, fs.du.interval=600000, hive.fetch.task.conversion.threshold=1073741824, dfs.client.file-block-storage-locations.num-threads=10, mapreduce.reduce.markreset.buffer.percent=0.0, mapreduce.shuffle.connection-keep-alive.timeout=5, hadoop.security.kms.client.encrypted.key.cache.size=500, hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager, hive.server2.map.fair.scheduler.queue=true, hadoop.registry.zk.root=/registry, s3.stream-buffer-size=4096, yarn.app.mapreduce.client.job.retry-interval=2000}
22/02/12 19:07:43 INFO DeltaSync: Hive Sync Conf => HiveSyncConfig{databaseName='default', tableName='customer_order_relation', baseFileFormat='PARQUET', hiveUser='hive', hivePass='hive', jdbcUrl='jdbc:hive2://hiveserver:10000', basePath='hdfs://namenode/user/admin/default/customer_order_relation/hudi', partitionFields=[pt], partitionValueExtractorClass='org.apache.hudi.hive.MultiPartKeysValueExtractor', assumeDatePartitioning=false, usePreApacheInputFormat=false, useJdbc=true, autoCreateDatabase=true, ignoreExceptions=false, skipROSuffix=false, useFileListingFromMetadata=false, tableProperties='null', serdeProperties='null', help=false, supportTimestamp=false, decodePartition=false, createManagedTable=false, syncAsSparkDataSourceTable=true, sparkSchemaLengthThreshold=4000, withOperationField=false, isConditionalSync=false}
22/02/12 19:07:43 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:43 INFO HoodieTableConfig: Loading table properties from hdfs://namenode/user/admin/default/customer_order_relation/hudi/.hoodie/hoodie.properties
22/02/12 19:07:43 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:43 INFO HoodieTableMetaClient: Loading Active commit timeline for hdfs://namenode/user/admin/default/customer_order_relation/hudi
22/02/12 19:07:43 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20220212190738514__commit__COMPLETED]}
22/02/12 19:07:43 INFO Utils: Supplied authorities: hiveserver:10000
22/02/12 19:07:43 INFO Utils: Resolved authority: hiveserver:10000
22/02/12 19:07:43 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hiveserver:10000
22/02/12 19:07:43 INFO QueryBasedDDLExecutor: Successfully established Hive connection to  jdbc:hive2://hiveserver:10000
22/02/12 19:07:43 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
22/02/12 19:07:43 INFO ObjectStore: ObjectStore, initialize called
22/02/12 19:07:43 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
22/02/12 19:07:43 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 506
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.28.150:54588 in memory (size: 121.9 KB, free: 365.8 MB)
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.21.0.14:44157 in memory (size: 121.9 KB, free: 365.8 MB)
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 445
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 426
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 428
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 462
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.28.150:54588 in memory (size: 86.2 KB, free: 365.9 MB)
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.21.0.14:44157 in memory (size: 86.2 KB, free: 365.9 MB)
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.28.150:54588 in memory (size: 121.9 KB, free: 366.0 MB)
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.21.0.14:44157 in memory (size: 121.9 KB, free: 366.0 MB)
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 362
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 519
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 458
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 520
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 460
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 430
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 404
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 495
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 456
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 439
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 386
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 451
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 496
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 512
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 521
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 517
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 425
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 490
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 390
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 410
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 474
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 412
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 429
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 358
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 421
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 359
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 423
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 469
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 514
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 476
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 472
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 481
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 467
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 470
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 484
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 438
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 356
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 402
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 441
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 400
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 480
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 415
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 477
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 525
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 444
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 466
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 483
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 433
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 528
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 473
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 395
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 399
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 468
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 405
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 388
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 475
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 491
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 394
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 373
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 498
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 419
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 471
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 368
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 401
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 509
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 510
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 370
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 403
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 518
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 448
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 427
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 447
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 457
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 375
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 372
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 411
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 501
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 494
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 502
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 416
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 455
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 454
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 396
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 479
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 392
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 380
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 440
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 365
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.28.150:54588 in memory (size: 84.2 KB, free: 366.1 MB)
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.21.0.14:44157 in memory (size: 84.2 KB, free: 366.1 MB)
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 369
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 420
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 389
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 364
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 449
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 391
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 371
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 488
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 503
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 355
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 381
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 465
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 431
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 387
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 379
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 408
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 434
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 508
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 397
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 450
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 442
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 516
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 377
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 478
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 524
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 382
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 513
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 414
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.28.150:54588 in memory (size: 26.1 KB, free: 366.1 MB)
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.21.0.14:44157 in memory (size: 26.1 KB, free: 366.1 MB)
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 437
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 482
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 515
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 493
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 526
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 378
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 446
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 398
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 499
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 436
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 376
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 492
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 363
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 418
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 393
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 435
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.28.150:54588 in memory (size: 26.2 KB, free: 366.2 MB)
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.21.0.14:44157 in memory (size: 26.2 KB, free: 366.2 MB)
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 453
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 406
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 523
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 522
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 463
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 443
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.28.150:54588 in memory (size: 121.9 KB, free: 366.3 MB)
22/02/12 19:07:43 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.21.0.14:44157 in memory (size: 121.9 KB, free: 366.3 MB)
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 360
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 366
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 367
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 489
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 497
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 383
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 452
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 407
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 486
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 424
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 417
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 505
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 413
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 504
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 374
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 507
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 357
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 422
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 529
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 384
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 459
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 385
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 487
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 461
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 409
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 485
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 464
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 527
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 361
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 511
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 500
22/02/12 19:07:43 INFO ContextCleaner: Cleaned accumulator 432
22/02/12 19:07:45 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
22/02/12 19:07:45 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
22/02/12 19:07:45 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
22/02/12 19:07:46 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
22/02/12 19:07:46 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
22/02/12 19:07:46 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
22/02/12 19:07:46 INFO ObjectStore: Initialized ObjectStore
22/02/12 19:07:46 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
22/02/12 19:07:46 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
22/02/12 19:07:46 INFO HiveMetaStore: Added admin role in metastore
22/02/12 19:07:46 INFO HiveMetaStore: Added public role in metastore
22/02/12 19:07:46 INFO HiveMetaStore: No user is added in admin role, since config is empty
22/02/12 19:07:46 INFO HiveMetaStore: 0: get_all_databases
22/02/12 19:07:46 INFO audit: ugi=mozhenghua	ip=unknown-ip-addr	cmd=get_all_databases	
22/02/12 19:07:46 INFO HiveMetaStore: 0: get_functions: db=default pat=*
22/02/12 19:07:46 INFO audit: ugi=mozhenghua	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
22/02/12 19:07:46 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
22/02/12 19:07:46 INFO metastore: Mestastore configuration hive.metastore.warehouse.dir changed from /user/hive/warehouse to file:/Users/mozhenghua/j2ee_solution/project/plugins/tis-datax/tis-datax-hudi-plugin/spark-warehouse
22/02/12 19:07:46 INFO metastore: Mestastore configuration hive.metastore.uris changed from  to thrift://hiveserver:9083
22/02/12 19:07:46 INFO HiveMetaStore: 0: Shutting down the object store...
22/02/12 19:07:46 INFO audit: ugi=mozhenghua	ip=unknown-ip-addr	cmd=Shutting down the object store...	
22/02/12 19:07:46 INFO HiveMetaStore: 0: Metastore shutdown complete.
22/02/12 19:07:46 INFO audit: ugi=mozhenghua	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
22/02/12 19:07:46 INFO metastore: Trying to connect to metastore with URI thrift://hiveserver:9083
22/02/12 19:07:46 INFO metastore: Connected to metastore.
22/02/12 19:07:46 INFO HiveSyncTool: Trying to sync hoodie table customer_order_relation with base path hdfs://namenode/user/admin/default/customer_order_relation/hudi of type COPY_ON_WRITE
22/02/12 19:07:47 INFO HiveSyncTool: No Schema difference for customer_order_relation
22/02/12 19:07:47 INFO HiveSyncTool: Schema sync complete. Syncing partitions for customer_order_relation
22/02/12 19:07:47 INFO HiveSyncTool: Last commit time synced was found to be 20220212142956973
22/02/12 19:07:47 INFO AbstractSyncHoodieClient: Last commit time synced is 20220212142956973, Getting commits since then
22/02/12 19:07:47 INFO HiveSyncTool: Storage partitions scan complete. Found 2
22/02/12 19:07:47 INFO HiveSyncTool: New Partitions []
22/02/12 19:07:47 INFO QueryBasedDDLExecutor: No partitions to add for customer_order_relation
22/02/12 19:07:47 INFO HiveSyncTool: Changed Partitions []
22/02/12 19:07:47 INFO QueryBasedDDLExecutor: No partitions to change for customer_order_relation
22/02/12 19:07:47 INFO HiveSyncTool: Sync complete for customer_order_relation
22/02/12 19:07:47 INFO DeltaSync: Shutting down embedded timeline server
22/02/12 19:07:47 INFO EmbeddedTimelineService: Closing Timeline server
22/02/12 19:07:47 INFO TimelineService: Closing Timeline Service
22/02/12 19:07:47 INFO Javalin: Stopping Javalin ...
22/02/12 19:07:47 INFO Javalin: Javalin has stopped
22/02/12 19:07:47 INFO RocksDbBasedFileSystemView: Closing Rocksdb !!
22/02/12 19:07:47 INFO RocksDBDAO: From Rocks DB : [db/db_impl.cc:365] Shutdown: canceling all background work
22/02/12 19:07:47 INFO RocksDBDAO: From Rocks DB : [db/db_impl.cc:521] Shutdown complete
22/02/12 19:07:47 INFO RocksDbBasedFileSystemView: Closed Rocksdb !!
22/02/12 19:07:47 INFO TimelineService: Closed Timeline Service
22/02/12 19:07:47 INFO EmbeddedTimelineService: Closed Timeline server
22/02/12 19:07:47 INFO HoodieDeltaStreamer: Shut down delta streamer
22/02/12 19:07:47 INFO SparkUI: Stopped Spark web UI at http://192.168.28.150:4040
22/02/12 19:07:47 INFO StandaloneSchedulerBackend: Shutting down all executors
22/02/12 19:07:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
22/02/12 19:07:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/02/12 19:07:47 INFO MemoryStore: MemoryStore cleared
22/02/12 19:07:47 INFO BlockManager: BlockManager stopped
22/02/12 19:07:47 INFO BlockManagerMaster: BlockManagerMaster stopped
22/02/12 19:07:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/02/12 19:07:47 INFO SparkContext: Successfully stopped SparkContext
22/02/12 19:07:47 INFO ShutdownHookManager: Shutdown hook called
22/02/12 19:07:47 INFO ShutdownHookManager: Deleting directory /private/var/folders/p_/5jfy_yk16kb_m48kn65vrqd40000gn/T/spark-ec853c64-2a7f-4c7e-894e-9b307fb19d26
22/02/12 19:07:47 INFO ShutdownHookManager: Deleting directory /private/var/folders/p_/5jfy_yk16kb_m48kn65vrqd40000gn/T/spark-ba1127d5-1e43-46e1-b730-bc3c559473f7
